{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivi-alencar/bachelor_thesis/blob/main/CLIP_UMAP_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a76e99-8d70-491f-ab66-aa65bde345f0",
      "metadata": {
        "id": "11a76e99-8d70-491f-ab66-aa65bde345f0"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97256d8b-df5d-4400-9f36-f8f002e2ea0f",
      "metadata": {
        "id": "97256d8b-df5d-4400-9f36-f8f002e2ea0f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67ce201-c11a-408c-8e09-db6bd416adbb",
      "metadata": {
        "id": "c67ce201-c11a-408c-8e09-db6bd416adbb"
      },
      "outputs": [],
      "source": [
        "import open_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae71311-59b1-46d0-be21-678748d3c557",
      "metadata": {
        "id": "3ae71311-59b1-46d0-be21-678748d3c557"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1aa7c1-d592-4cfb-8253-9a3655d02d00",
      "metadata": {
        "id": "ef1aa7c1-d592-4cfb-8253-9a3655d02d00"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6522db-9c8c-47b5-a4db-467de3b1f703",
      "metadata": {
        "id": "4b6522db-9c8c-47b5-a4db-467de3b1f703"
      },
      "outputs": [],
      "source": [
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0987138-8610-49d4-9240-ec937a9ca04a",
      "metadata": {
        "id": "f0987138-8610-49d4-9240-ec937a9ca04a"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54203e69-f895-4d73-9b54-42fe399014a2",
      "metadata": {
        "id": "54203e69-f895-4d73-9b54-42fe399014a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c3fd1e-57eb-49f7-a9d3-7d19dfea9b47",
      "metadata": {
        "id": "15c3fd1e-57eb-49f7-a9d3-7d19dfea9b47"
      },
      "outputs": [],
      "source": [
        "from adjustText import adjust_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a51dc1-a095-43d6-befb-8390b8cc8b5d",
      "metadata": {
        "id": "42a51dc1-a095-43d6-befb-8390b8cc8b5d"
      },
      "outputs": [],
      "source": [
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872f96c9-ddd3-4ab4-a165-48322cd688f1",
      "metadata": {
        "id": "872f96c9-ddd3-4ab4-a165-48322cd688f1"
      },
      "outputs": [],
      "source": [
        "image_folder = \"path to image folder\" # Image folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e668e3b-d36f-4e35-9cc1-225ce732da63",
      "metadata": {
        "id": "5e668e3b-d36f-4e35-9cc1-225ce732da63"
      },
      "outputs": [],
      "source": [
        "image_files = os.listdir(image_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5359249a-4564-4a79-b251-b6a6d47c888f",
      "metadata": {
        "id": "5359249a-4564-4a79-b251-b6a6d47c888f"
      },
      "outputs": [],
      "source": [
        "coco = COCO('path to instances_val2017.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4d7d6a-44ca-4340-ba73-c7e5b8393d54",
      "metadata": {
        "id": "dc4d7d6a-44ca-4340-ba73-c7e5b8393d54"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of to map category IDs to names\n",
        "category_id_to_name = {category['id']: category['name'] for category in coco.loadCats(coco.getCatIds())}\n",
        "class_queries = [f\"a photo of a {name}\" for name in category_id_to_name.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4add9668-6ca1-46ff-bb49-0034d9fbdef4",
      "metadata": {
        "id": "4add9668-6ca1-46ff-bb49-0034d9fbdef4"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model, tokenizer, and preprocessing function\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "model.eval()  # Set to evaluation mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9125f2-5c8a-4e0a-bf72-c2dcabb837e5",
      "metadata": {
        "id": "2b9125f2-5c8a-4e0a-bf72-c2dcabb837e5"
      },
      "outputs": [],
      "source": [
        "# Get all image IDs from COCO\n",
        "image_ids = coco.getImgIds()\n",
        "image_embeddings = []\n",
        "\n",
        "# Iterate over each image\n",
        "for image_id in tqdm(image_ids):\n",
        "    # Get image file name from COCO\n",
        "    image_info = coco.loadImgs(image_id)[0]\n",
        "    image_path = f\"{image_folder}/{image_info['file_name']}\"\n",
        "\n",
        "    # Open and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate the image embedding\n",
        "    with torch.no_grad():\n",
        "        image_embedding = model.encode_image(image_input)\n",
        "    image_embeddings.append(image_embedding.cpu())  # Store on CPU to save GPU memory\n",
        "\n",
        "# Stack all embeddings into a single tensor for later use\n",
        "image_embeddings_tensor = torch.cat(image_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0db878-fc74-4667-939e-b8a42a2d27bb",
      "metadata": {
        "id": "5e0db878-fc74-4667-939e-b8a42a2d27bb"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store image IDs and corresponding categories\n",
        "image_ids_list = []\n",
        "image_classes_list = []\n",
        "\n",
        "# Populate lists with image IDs and the categories for each image\n",
        "for image_id in image_ids:\n",
        "    # Get all categories (annotations) for the image\n",
        "    annotations = coco.loadAnns(coco.getAnnIds(imgIds=image_id))\n",
        "    classes = {category_id_to_name[ann['category_id']] for ann in annotations}\n",
        "    image_ids_list.append(image_id)\n",
        "    image_classes_list.append(\",\".join(sorted(classes)))\n",
        "\n",
        "# Create the DataFrame\n",
        "coco_df = pd.DataFrame({\n",
        "    'image_id': image_ids_list,\n",
        "    'classes': image_classes_list\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326d2633-bdc1-46ce-a376-f79142999456",
      "metadata": {
        "id": "326d2633-bdc1-46ce-a376-f79142999456"
      },
      "outputs": [],
      "source": [
        "# Create text embeddings\n",
        "text_embeddings = []\n",
        "for query in class_queries:\n",
        "    # Tokenize and encode text\n",
        "    text_tokens = open_clip.tokenize([query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_embedding = model.encode_text(text_tokens)\n",
        "    text_embeddings.append(text_embedding.cpu())\n",
        "\n",
        "# Stack all text embeddings into a single tensor\n",
        "text_embeddings_tensor = torch.cat(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a2b2e2-4c8c-4fdf-a4f8-7c86ec9379a0",
      "metadata": {
        "id": "16a2b2e2-4c8c-4fdf-a4f8-7c86ec9379a0"
      },
      "outputs": [],
      "source": [
        "print(\"Image embeddings shape:\", image_embeddings_tensor.shape)  # Expected: [num_images, 512]\n",
        "print(\"Text embeddings shape:\", text_embeddings_tensor.shape)  # Expected: [num_categories, 512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea049d3e-1db0-423d-9f76-3ccad8530e63",
      "metadata": {
        "id": "ea049d3e-1db0-423d-9f76-3ccad8530e63"
      },
      "outputs": [],
      "source": [
        "# Normalize the image embeddings\n",
        "image_embeddings_normalized = F.normalize(image_embeddings_tensor, p=2, dim=1)\n",
        "\n",
        "# Normalize the text embeddings\n",
        "text_embeddings_normalized = F.normalize(text_embeddings_tensor, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d68073-33d2-4d7a-8efa-ccabe5f8923b",
      "metadata": {
        "id": "67d68073-33d2-4d7a-8efa-ccabe5f8923b"
      },
      "outputs": [],
      "source": [
        "# Convert normalized embeddings to Numpy arrays\n",
        "image_embeddings_np = image_embeddings_normalized.cpu().numpy()\n",
        "text_embeddings_np = text_embeddings_normalized.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcfc6c02-4a5e-4021-bf57-7116055eb0a9",
      "metadata": {
        "id": "dcfc6c02-4a5e-4021-bf57-7116055eb0a9"
      },
      "source": [
        "## Use UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2765a720-8974-4168-a861-c2c2d9c33f3b",
      "metadata": {
        "id": "2765a720-8974-4168-a861-c2c2d9c33f3b"
      },
      "outputs": [],
      "source": [
        "# Initialize UMAP with parameters that often work well for image embeddings\n",
        "umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42, metric='cosine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6f334e-f8c0-4048-ab35-fdc0cf4163be",
      "metadata": {
        "id": "6d6f334e-f8c0-4048-ab35-fdc0cf4163be"
      },
      "outputs": [],
      "source": [
        "umap_image_embeddings = umap_reducer.fit_transform(image_embeddings_np)\n",
        "print(\"UMAP transformation completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb25641-570b-471f-a1cc-d97f9b3d8fa9",
      "metadata": {
        "id": "edb25641-570b-471f-a1cc-d97f9b3d8fa9"
      },
      "outputs": [],
      "source": [
        "## No colors\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(umap_image_embeddings[:, 0], umap_image_embeddings[:, 1], s=5, alpha=0.7)\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP Projection of Image Embeddings\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204f6764-6094-45b3-9157-3c0f84fe104f",
      "metadata": {
        "id": "204f6764-6094-45b3-9157-3c0f84fe104f"
      },
      "outputs": [],
      "source": [
        "# Define classes and their corresponding colors\n",
        "classes_to_highlight = {\n",
        "    'skis': 'blue',\n",
        "    'elephant': 'green',\n",
        "    'airplane': 'purple',\n",
        "    'sports ball': 'orange',\n",
        "    'giraffe': 'red',\n",
        "    'surfboard': 'pink',\n",
        "    'train': 'cyan',\n",
        "    'boat': 'navy',\n",
        "    #'elephant': 'brown'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9df0e5-1a16-4572-a9ea-daaf1a0b1ed4",
      "metadata": {
        "id": "3f9df0e5-1a16-4572-a9ea-daaf1a0b1ed4"
      },
      "outputs": [],
      "source": [
        "# Plot all images in a light color\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(umap_image_embeddings[:, 0], umap_image_embeddings[:, 1], s=5, alpha=0.3, color='gray', label='Other Images')\n",
        "\n",
        "# Plot each selected class with its unique color\n",
        "for class_name, color in classes_to_highlight.items():\n",
        "    class_df = coco_df[coco_df['classes'].str.contains(class_name)]\n",
        "    class_indices = class_df.index.tolist()\n",
        "    class_umap_embeddings = umap_image_embeddings[class_indices]\n",
        "\n",
        "    plt.scatter(class_umap_embeddings[:, 0], class_umap_embeddings[:, 1], s=10, alpha=0.7, color=color, label=f'{class_name.capitalize()} Images')\n",
        "\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"UMAP - Image Embeddings (ViT-B/32)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702b9e3e-d967-48cd-9157-a0664c059a4c",
      "metadata": {
        "id": "702b9e3e-d967-48cd-9157-a0664c059a4c"
      },
      "outputs": [],
      "source": [
        "## Filter images by category\n",
        "giraffe_df = coco_df[coco_df['classes'].str.contains('giraffe')]\n",
        "giraffe_indices = giraffe_df.index.tolist()  # Get indices of images with \"giraffe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993a8752-fb53-4647-a39b-4752c2714979",
      "metadata": {
        "id": "993a8752-fb53-4647-a39b-4752c2714979"
      },
      "outputs": [],
      "source": [
        "## Separate embeddings\n",
        "giraffe_umap_image_embedding = umap_image_embeddings[giraffe_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0ea754-3c6a-48f0-9826-465d49351532",
      "metadata": {
        "id": "be0ea754-3c6a-48f0-9826-465d49351532"
      },
      "outputs": [],
      "source": [
        "## Visualize with specific color\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot all images in a light color\n",
        "plt.scatter(umap_image_embeddings[:, 0], umap_image_embeddings[:, 1], s=5, alpha=0.3, color='gray', label='Other Images')\n",
        "\n",
        "# Plot only \"giraffe\" images in a distinct color\n",
        "plt.scatter(giraffe_umap_image_embedding[:, 0], giraffe_umap_image_embedding[:, 1], s=10, alpha=0.7, color='red', label='Giraffe Images')\n",
        "\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP Projection of Image Embeddings (Giraffe Highlighted)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9380f470-de3d-4e1d-9a4d-1f64199cab61",
      "metadata": {
        "id": "9380f470-de3d-4e1d-9a4d-1f64199cab61"
      },
      "source": [
        "## UMAP with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da67aff2-2ee0-44ea-812d-230e2f426a39",
      "metadata": {
        "id": "da67aff2-2ee0-44ea-812d-230e2f426a39"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to reduce dimensionality, then use UMAP\n",
        "pca = PCA(n_components=50)  # You can try 50, 100, or another value less than 256\n",
        "reduced_embeddings = pca.fit_transform(image_embeddings_np)\n",
        "umap_reducer = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.4, random_state=42, metric=\"cosine\")\n",
        "umap_image_embeddings = umap_reducer.fit_transform(reduced_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191a5cea-1367-432b-87f6-a46f77600833",
      "metadata": {
        "id": "191a5cea-1367-432b-87f6-a46f77600833"
      },
      "outputs": [],
      "source": [
        "## Visualize with specific color\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot all images in a light color\n",
        "plt.scatter(umap_image_embeddings[:, 0], umap_image_embeddings[:, 1], s=5, alpha=0.3, color='gray', label='Other Images')\n",
        "\n",
        "# Plot only \"giraffe\" images in a distinct color\n",
        "plt.scatter(giraffe_umap_image_embedding[:, 0], giraffe_umap_image_embedding[:, 1], s=10, alpha=0.7, color='red', label='Giraffe Images')\n",
        "\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP Projection of Image Embeddings (Giraffe Highlighted)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb7af01-2e60-405c-976a-fe71b0ef44e8",
      "metadata": {
        "id": "8eb7af01-2e60-405c-976a-fe71b0ef44e8"
      },
      "source": [
        "## Text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80137e4-c202-4950-bcb1-becdcb0b0cf6",
      "metadata": {
        "id": "c80137e4-c202-4950-bcb1-becdcb0b0cf6"
      },
      "outputs": [],
      "source": [
        "# Initialize UMAP with parameters suited for smaller datasets\n",
        "umap_reducer = umap.UMAP(n_components=2, n_neighbors=5, min_dist=0.3, random_state=42, metric=\"cosine\")\n",
        "umap_text_embeddings = umap_reducer.fit_transform(text_embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f2ca2b-4bee-448a-878e-4967212aad7e",
      "metadata": {
        "id": "63f2ca2b-4bee-448a-878e-4967212aad7e"
      },
      "outputs": [],
      "source": [
        "# Plot UMAP embeddings with labels\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(umap_text_embeddings[:, 0], umap_text_embeddings[:, 1], s=50, alpha=0.7)\n",
        "\n",
        "# Create text annotations for each point and store them in a list\n",
        "texts = []\n",
        "for i, category_name in enumerate(category_id_to_name.values()):\n",
        "    texts.append(\n",
        "        plt.text(umap_text_embeddings[i, 0], umap_text_embeddings[i, 1], category_name, fontsize=10, ha='center')\n",
        "    )\n",
        "\n",
        "# Adjust text to reduce overlap\n",
        "adjust_text(texts, only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle=\"->\", color='grey', lw=0.5))\n",
        "\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP Projection of COCO Category Text Embeddings\")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cac992-24dd-415d-a203-1bf79e4206a0",
      "metadata": {
        "id": "46cac992-24dd-415d-a203-1bf79e4206a0"
      },
      "source": [
        "### Attempt to color groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88f9006-0858-448c-b8a1-37770e98f89d",
      "metadata": {
        "id": "b88f9006-0858-448c-b8a1-37770e98f89d"
      },
      "outputs": [],
      "source": [
        "# Category to group mapping\n",
        "category_to_group = {\n",
        "    \"airplane\": \"vehicle\",\n",
        "    \"apple\": \"food\",\n",
        "    \"backpack\": \"accessory\",\n",
        "    \"banana\": \"food\",\n",
        "    \"baseball glove\": \"sport\",\n",
        "    \"baseball bat\": \"sport\",\n",
        "    \"bear\": \"animal\",\n",
        "    \"bed\": \"comfort\",\n",
        "    \"bench\": \"comfort\",\n",
        "    \"bicycle\": \"vehicle\",\n",
        "    \"bird\": \"animal\",\n",
        "    \"boat\": \"vehicle\",\n",
        "    #\"book\": \"XXX\",\n",
        "    \"bottle\": \"furniture\",\n",
        "    \"bowl\": \"furniture\",\n",
        "    \"broccoli\": \"food\",\n",
        "    \"bus\": \"vehicle\",\n",
        "    \"cake\": \"food\",\n",
        "    \"car\": \"vehicle\",\n",
        "    \"carrot\": \"food\",\n",
        "    \"cat\": \"animal\",\n",
        "    \"cell phone\": \"appliance\",\n",
        "    \"chair\": \"comfort\",\n",
        "    #\"clock\": \"XXX\",\n",
        "    \"couch\": \"comfort\",\n",
        "    \"cow\": \"animal\",\n",
        "    \"cup\": \"furniture\",\n",
        "    \"dining table\": \"furniture\",\n",
        "    \"dog\": \"animal\",\n",
        "    \"donut\": \"food\",\n",
        "    \"elephant\": \"animal\",\n",
        "    \"fire hydrant\": \"street\",\n",
        "    \"fork\": \"cutlery\",\n",
        "    \"frisbee\": \"sport\",\n",
        "    \"giraffe\": \"animal\",\n",
        "    \"hair drier\": \"appliance\",\n",
        "    \"handbag\": \"accessory\",\n",
        "    \"horse\": \"animal\",\n",
        "    \"hot dog\": \"food\",\n",
        "    \"keyboard\": \"appliance\",\n",
        "    \"kite\": \"sport\",\n",
        "    \"knife\": \"cutlery\",\n",
        "    \"laptop\": \"appliance\",\n",
        "    \"microwave\": \"appliance\",\n",
        "    \"motorcycle\": \"vehicle\",\n",
        "    \"mouse\": \"appliance\",\n",
        "    \"orange\": \"food\",\n",
        "    \"oven\": \"appliance\",\n",
        "    \"parking meter\": \"street\",\n",
        "    #\"person\": \"person\",\n",
        "    \"pizza\": \"food\",\n",
        "    \"potted plant\": \"furniture\",\n",
        "    \"refrigerator\": \"appliance\",\n",
        "    \"remote\": \"appliance\",\n",
        "    \"sandwich\": \"food\",\n",
        "    \"scissors\": \"cutlery\",\n",
        "    \"sheep\": \"animal\",\n",
        "    \"sink\": \"furniture\",\n",
        "    \"skateboard\": \"sport\",\n",
        "    \"skis\": \"sport\",\n",
        "    \"snowboard\": \"sport\",\n",
        "    \"spoon\": \"cutlery\",\n",
        "    \"sports ball\": \"sport\",\n",
        "    \"stop sign\": \"street\",\n",
        "    \"suitcase\": \"accessory\",\n",
        "    \"surfboard\": \"sport\",\n",
        "    #\"teddy bear\": \"XXX\",\n",
        "    \"tennis racket\": \"sport\",\n",
        "    #\"tie\": \"XXX\",\n",
        "    \"toaster\": \"appliance\",\n",
        "    \"toilet\": \"furniture\",\n",
        "    #\"toothbrush\": \"XXX\",\n",
        "    \"traffic light\": \"street\",\n",
        "    \"train\": \"vehicle\",\n",
        "    \"truck\": \"vehicle\",\n",
        "    \"tv\": \"appliance\",\n",
        "    #\"umbrella\": \"XXX\",\n",
        "    \"vase\": \"furniture\",\n",
        "    \"wine glass\": \"furniture\",\n",
        "    \"zebra\": \"animal\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a203df6-2de3-4cbb-8700-35697700ad80",
      "metadata": {
        "id": "1a203df6-2de3-4cbb-8700-35697700ad80"
      },
      "outputs": [],
      "source": [
        "# Group to color mapping\n",
        "group_to_color = {\n",
        "    'vehicle': 'blue',\n",
        "    'food': 'orange',\n",
        "    'accessory': 'green',\n",
        "    'sport': 'purple',\n",
        "    'animal': 'red',\n",
        "    'furniture': 'pink',\n",
        "    'street': 'cyan',\n",
        "    'cutlery': 'olive',\n",
        "    'comfort': 'brown',\n",
        "    'appliance': 'navy',\n",
        "    'undefined': 'gray',  # Default color for undefined categories\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40cd98e-ecf3-458c-8efe-e9e1caf04fe6",
      "metadata": {
        "id": "f40cd98e-ecf3-458c-8efe-e9e1caf04fe6"
      },
      "outputs": [],
      "source": [
        "# Map category names to their corresponding colors\n",
        "category_colors = []\n",
        "for category in category_id_to_name.values():\n",
        "    # Assign group or default to 'undefined'\n",
        "    group = category_to_group.get(category, 'undefined')\n",
        "    # Get the corresponding color\n",
        "    category_colors.append(group_to_color[group])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62326bae-6790-44aa-8c8a-6d0d09391c6d",
      "metadata": {
        "id": "62326bae-6790-44aa-8c8a-6d0d09391c6d"
      },
      "outputs": [],
      "source": [
        "# Plot UMAP embeddings with labels and colors\n",
        "plt.figure(figsize=(12, 10))\n",
        "scatter = plt.scatter(\n",
        "    umap_text_embeddings[:, 0],\n",
        "    umap_text_embeddings[:, 1],\n",
        "    s=50,\n",
        "    c=category_colors,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# Create text annotations for each point and store them in a list\n",
        "texts = []\n",
        "for i, category_name in enumerate(category_id_to_name.values()):\n",
        "    texts.append(\n",
        "        plt.text(umap_text_embeddings[i, 0], umap_text_embeddings[i, 1], category_name, fontsize=10, ha='center')\n",
        "    )\n",
        "\n",
        "# Adjust text to reduce overlap\n",
        "adjust_text(texts, only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle=\"->\", color='grey', lw=0.5))\n",
        "\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP - COCO Category Text Embeddings (ViT-B/32)\")\n",
        "\n",
        "# Create a legend\n",
        "handles = [plt.Line2D([0], [0], marker='o', color=color, markersize=10, linestyle='', label=group)\n",
        "           for group, color in group_to_color.items()]\n",
        "plt.legend(\n",
        "    handles=handles,\n",
        "    title=\"Groups\",\n",
        "    loc='best',\n",
        "    prop={'size': 9},  # Adjust font size of the legend\n",
        "    markerscale=0.7  # Adjust size of the legend markers\n",
        ")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a844e2e-1be1-4b68-b1be-84e43f071b18",
      "metadata": {
        "id": "4a844e2e-1be1-4b68-b1be-84e43f071b18"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "796a86c6-b1b0-4856-9e2a-60d6a9ee4da2",
      "metadata": {
        "id": "796a86c6-b1b0-4856-9e2a-60d6a9ee4da2"
      },
      "source": [
        "## Try UMAP - image and text embeddings together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d720e8b4-61af-40c3-a59d-7845c21daf64",
      "metadata": {
        "id": "d720e8b4-61af-40c3-a59d-7845c21daf64"
      },
      "outputs": [],
      "source": [
        "## We need to calculate image and text visualizations in one pass\n",
        "combined_embeddings = np.vstack([image_embeddings_np, text_embeddings_np])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c170309-0df6-49b4-b8f6-c60a6e509634",
      "metadata": {
        "id": "9c170309-0df6-49b4-b8f6-c60a6e509634"
      },
      "outputs": [],
      "source": [
        "## Run UMAP on combined embeddings\n",
        "umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
        "combined_umap_embeddings = umap_reducer.fit_transform(combined_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac6616d-b9a9-481d-a774-d8c262b4736b",
      "metadata": {
        "id": "cac6616d-b9a9-481d-a774-d8c262b4736b"
      },
      "outputs": [],
      "source": [
        "# Separate the transformed embeddings\n",
        "num_images = image_embeddings_np.shape[0]\n",
        "umap_image_embeddings = combined_umap_embeddings[:num_images]\n",
        "umap_text_embeddings = combined_umap_embeddings[num_images:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de7a6b5-3a62-499e-9949-bcb85e3f54dd",
      "metadata": {
        "id": "1de7a6b5-3a62-499e-9949-bcb85e3f54dd"
      },
      "outputs": [],
      "source": [
        "giraffe_index = list(category_id_to_name.values()).index(\"giraffe\")\n",
        "umap_giraffe_images = umap_image_embeddings[giraffe_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c45eb1-8e18-4ddd-b68b-e9884228d972",
      "metadata": {
        "id": "56c45eb1-8e18-4ddd-b68b-e9884228d972"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot all other image embeddings in a base color\n",
        "plt.scatter(umap_image_embeddings[:, 0], umap_image_embeddings[:, 1], s=5, alpha=0.7, color='blue', label=\"Images\")\n",
        "\n",
        "# Plot all other text embeddings in a base marker/label style\n",
        "plt.scatter(umap_text_embeddings[:, 0], umap_text_embeddings[:, 1], s=50, alpha=0.9, color='red', label=\"Categories\")\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.title(\"2D UMAP of Image and Text Embeddings (ViT-B/32)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dac012d-daf6-4f75-8b43-dad350508c05",
      "metadata": {
        "id": "2dac012d-daf6-4f75-8b43-dad350508c05"
      },
      "source": [
        "## Try PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0771f395-4a7f-464b-a98b-06ecc52f328f",
      "metadata": {
        "id": "0771f395-4a7f-464b-a98b-06ecc52f328f"
      },
      "source": [
        "### Text only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d514f0-6369-4fda-943d-37d21344d552",
      "metadata": {
        "id": "90d514f0-6369-4fda-943d-37d21344d552"
      },
      "outputs": [],
      "source": [
        "text_embeddings_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33a265c-8ada-4728-8397-65e84646a952",
      "metadata": {
        "id": "b33a265c-8ada-4728-8397-65e84646a952"
      },
      "outputs": [],
      "source": [
        "# To decide the number of components, do variance calculations\n",
        "# Perform PCA with enough components to capture most of the variance\n",
        "text_pca = PCA(n_components=50)  # Adjust this to a high enough number\n",
        "text_pca.fit(text_embeddings_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790a9685-db70-4b84-b3b9-964a30a2bf4c",
      "metadata": {
        "id": "790a9685-db70-4b84-b3b9-964a30a2bf4c"
      },
      "source": [
        "#### Explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ef5853-eaba-404b-80e3-21a556691cd5",
      "metadata": {
        "id": "41ef5853-eaba-404b-80e3-21a556691cd5"
      },
      "outputs": [],
      "source": [
        "# Plot individual explained variance for each component\n",
        "text_explained_variance = text_pca.explained_variance_ratio_\n",
        "print(\"Variance explained by each PCA component:\", text_explained_variance)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.bar(range(1, len(text_explained_variance) + 1), text_explained_variance, alpha=0.7)\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.title(\"Explained Variance - Text (ViT-B/32)\")\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffab184-0b84-45fa-8069-e8041ad1d252",
      "metadata": {
        "id": "1ffab184-0b84-45fa-8069-e8041ad1d252"
      },
      "source": [
        "#### Cumulative variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be7ba3a-901c-4d10-81b7-e81ecb3dd6ee",
      "metadata": {
        "id": "5be7ba3a-901c-4d10-81b7-e81ecb3dd6ee"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative explained variance\n",
        "text_cumulative_variance = np.cumsum(text_pca.explained_variance_ratio_)\n",
        "print(\"Cumulative variance explained:\", text_cumulative_variance)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(text_cumulative_variance) + 1), text_cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Cumulative Explained Variance - Text (ViT-B/32)\")\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac72d4e-3725-45fc-8f43-9700c6263ad7",
      "metadata": {
        "id": "3ac72d4e-3725-45fc-8f43-9700c6263ad7"
      },
      "outputs": [],
      "source": [
        "# Plot both cumulative and individual explained variance\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(text_cumulative_variance) + 1), text_cumulative_variance, marker='o', linestyle='--', label='Cumulative Explained Variance')\n",
        "plt.bar(range(1, len(text_explained_variance) + 1), text_explained_variance, alpha=0.6, label='Individual Explained Variance')\n",
        "\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.title(\"Explained Variance - Text (ViT-B/32)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b049f8e6-2fcf-4a64-ab0f-d88531481516",
      "metadata": {
        "id": "b049f8e6-2fcf-4a64-ab0f-d88531481516"
      },
      "outputs": [],
      "source": [
        "# Perform PCA transformation\n",
        "text_embeddings_pca = text_pca.fit_transform(text_embeddings_np)\n",
        "\n",
        "# Create the PCA scatter plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(text_embeddings_pca[:, 0], text_embeddings_pca[:, 1], s=50, alpha=0.7)\n",
        "\n",
        "# Annotate points with category names using adjust_text\n",
        "texts = []\n",
        "for i, category_name in enumerate(category_id_to_name.values()):\n",
        "    texts.append(\n",
        "        plt.text(text_embeddings_pca[i, 0], text_embeddings_pca[i, 1], category_name, fontsize=10, ha='center')\n",
        "    )\n",
        "\n",
        "# Adjust text to reduce overlap\n",
        "adjust_text(texts, only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle=\"->\", color='grey', lw=0.5))\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"2D PCA Projection of COCO Category Text Embeddings\")\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6a47b6-2b19-4bd6-9d0a-642bf11f8769",
      "metadata": {
        "id": "bd6a47b6-2b19-4bd6-9d0a-642bf11f8769"
      },
      "source": [
        "### Attempt to color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aecd7d06-90e4-42f3-b553-897b3651386a",
      "metadata": {
        "id": "aecd7d06-90e4-42f3-b553-897b3651386a"
      },
      "outputs": [],
      "source": [
        "# Create the PCA scatter plot with category colors\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(\n",
        "    text_embeddings_pca[:, 0],\n",
        "    text_embeddings_pca[:, 1],\n",
        "    s=50,\n",
        "    c=category_colors,  # Apply colors\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# Annotate points with category names using adjust_text\n",
        "texts = []\n",
        "for i, category_name in enumerate(category_id_to_name.values()):\n",
        "    texts.append(\n",
        "        plt.text(text_embeddings_pca[i, 0], text_embeddings_pca[i, 1], category_name, fontsize=10, ha='center')\n",
        "    )\n",
        "\n",
        "# Adjust text to reduce overlap\n",
        "adjust_text(texts, only_move={'points': 'y', 'text': 'xy'}, arrowprops=dict(arrowstyle=\"->\", color='grey', lw=0.5))\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"2D PCA - COCO Category Text Embeddings (ViT-B/32)\")\n",
        "\n",
        "# Create a legend\n",
        "handles = [plt.Line2D([0], [0], marker='o', color=color, markersize=10, linestyle='', label=group)\n",
        "           for group, color in group_to_color.items()]\n",
        "plt.legend(\n",
        "    handles=handles,\n",
        "    title=\"Groups\",\n",
        "    loc='best',\n",
        "    prop={'size': 9},  # Adjust font size of the legend\n",
        "    markerscale=0.7  # Adjust size of the legend markers\n",
        ")\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485ad7e6-a17a-4ca4-aa7b-cbc26a35d314",
      "metadata": {
        "id": "485ad7e6-a17a-4ca4-aa7b-cbc26a35d314"
      },
      "source": [
        "### Image only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086f0f42-088d-4f84-9dd1-08c58acc0832",
      "metadata": {
        "id": "086f0f42-088d-4f84-9dd1-08c58acc0832"
      },
      "outputs": [],
      "source": [
        "# Perform PCA with enough components to capture most of the variance\n",
        "image_pca = PCA(n_components=50)  # Adjust this to a high enough number\n",
        "image_pca.fit(image_embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cce5b91-1d42-43f9-8aaa-3bbf39e97db1",
      "metadata": {
        "id": "2cce5b91-1d42-43f9-8aaa-3bbf39e97db1"
      },
      "outputs": [],
      "source": [
        "# Plot individual explained variance for each component\n",
        "image_explained_variance = image_pca.explained_variance_ratio_\n",
        "print(\"Variance explained by each PCA component:\", image_explained_variance)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.bar(range(1, len(image_explained_variance) + 1), image_explained_variance, alpha=0.7)\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.title(\"Explained Variance - Image (ViT-B/32)\")\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f40d87d-6faf-4353-be6a-353a181462b1",
      "metadata": {
        "id": "1f40d87d-6faf-4353-be6a-353a181462b1"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative explained variance\n",
        "image_cumulative_variance = np.cumsum(image_pca.explained_variance_ratio_)\n",
        "print(\"Cumulative variance explained:\", image_cumulative_variance)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(image_cumulative_variance) + 1), image_cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Cumulative Explained Variance - Image (ViT-B/32)\")\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0265eee-ad64-4676-8065-1f053b1d9db8",
      "metadata": {
        "id": "a0265eee-ad64-4676-8065-1f053b1d9db8"
      },
      "outputs": [],
      "source": [
        "# Plot both cumulative and individual explained variance\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(range(1, len(image_cumulative_variance) + 1), image_cumulative_variance, marker='o', linestyle='--', label='Cumulative Explained Variance')\n",
        "plt.bar(range(1, len(image_explained_variance) + 1), image_explained_variance, alpha=0.6, label='Individual Explained Variance')\n",
        "\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.title(\"Explained Variance - Image (ViT-B/32)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d00758a-1292-46d4-ab41-ec0e7360b6e9",
      "metadata": {
        "id": "9d00758a-1292-46d4-ab41-ec0e7360b6e9"
      },
      "outputs": [],
      "source": [
        "# Plot PCA without colors\n",
        "image_embeddings_pca = image_pca.fit_transform(image_embeddings_np)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(image_embeddings_pca[:, 0], image_embeddings_pca[:, 1], s=5, alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"2D PCA of COCO Image Embeddings\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f0f50a-af61-42b3-8734-7bf81274001c",
      "metadata": {
        "id": "56f0f50a-af61-42b3-8734-7bf81274001c"
      },
      "outputs": [],
      "source": [
        "# Plot all images in a light color\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(image_embeddings_pca[:, 0], image_embeddings_pca[:, 1], s=5, alpha=0.3, color='gray', label='Other Images')\n",
        "\n",
        "# Plot each selected class with its unique color\n",
        "for class_name, color in classes_to_highlight.items():\n",
        "    # Find indices for the class\n",
        "    class_df = coco_df[coco_df['classes'].str.contains(class_name)]\n",
        "    class_indices = class_df.index.tolist()\n",
        "\n",
        "    # Get the PCA embeddings for the class\n",
        "    class_pca_embeddings = image_embeddings_pca[class_indices]\n",
        "\n",
        "    # Plot the points for the class\n",
        "    plt.scatter(class_pca_embeddings[:, 0], class_pca_embeddings[:, 1],\n",
        "                s=10, alpha=0.7, color=color, label=f'{class_name.capitalize()} Images')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"2D PCA of COCO Image Embeddings (ViT-B/32)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d190ed3-bffd-4e56-b3db-cf058c2a17fc",
      "metadata": {
        "id": "2d190ed3-bffd-4e56-b3db-cf058c2a17fc"
      },
      "source": [
        "## Try t-SNE with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35423489-396b-457e-8167-5547793920a4",
      "metadata": {
        "id": "35423489-396b-457e-8167-5547793920a4"
      },
      "outputs": [],
      "source": [
        "# Initialize t-SNE and reduce PCA-transformed data to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "image_embeddings_tsnePCA = tsne.fit_transform(image_embeddings_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ce538b-3566-426c-99a6-52d6cf826b73",
      "metadata": {
        "id": "12ce538b-3566-426c-99a6-52d6cf826b73"
      },
      "outputs": [],
      "source": [
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(image_embeddings_tsnePCA[:, 0], image_embeddings_tsnePCA[:, 1], s=5, alpha=0.7)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"2D t-SNE of Image Embeddings\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b587b83e-1059-4055-a5ae-a6127521d7b2",
      "metadata": {
        "id": "b587b83e-1059-4055-a5ae-a6127521d7b2"
      },
      "outputs": [],
      "source": [
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(image_embeddings_tsnePCA[:, 0], image_embeddings_tsnePCA[:, 1], s=5, alpha=0.3, color='gray', label='Other Images')\n",
        "\n",
        "# Plot each selected class with its unique color\n",
        "for class_name, color in classes_to_highlight.items():\n",
        "    class_df = coco_df[coco_df['classes'].str.contains(class_name)]\n",
        "    class_indices = class_df.index.tolist()\n",
        "    class_tsnePCA_embeddings = image_embeddings_tsnePCA[class_indices]\n",
        "\n",
        "    plt.scatter(class_tsnePCA_embeddings[:, 0], class_tsnePCA_embeddings[:, 1], s=10, alpha=0.7, color=color, label=f'{class_name.capitalize()} Images')\n",
        "\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"2D t-SNE Projection of PCA-transformed Image Embeddings\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4ee718-daf3-4128-8049-5f8ef8d06e33",
      "metadata": {
        "id": "5b4ee718-daf3-4128-8049-5f8ef8d06e33"
      },
      "source": [
        "# PCA GAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b995383f-76d0-4763-9aae-a91a2183b6a4",
      "metadata": {
        "id": "b995383f-76d0-4763-9aae-a91a2183b6a4"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to the combined embeddings\n",
        "pca = PCA(n_components=2)\n",
        "combined_pca_embeddings = pca.fit_transform(combined_embeddings)\n",
        "\n",
        "# Separate the transformed embeddings\n",
        "num_images = image_embeddings_np.shape[0]\n",
        "pca_image_embeddings = combined_pca_embeddings[:num_images]\n",
        "pca_text_embeddings = combined_pca_embeddings[num_images:]\n",
        "\n",
        "# Plot the PCA-transformed embeddings\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot the image embeddings in blue\n",
        "plt.scatter(pca_image_embeddings[:, 0], pca_image_embeddings[:, 1], s=5, alpha=0.7, color='blue', label=\"Images\")\n",
        "\n",
        "# Plot the text embeddings in red\n",
        "plt.scatter(pca_text_embeddings[:, 0], pca_text_embeddings[:, 1], s=50, alpha=0.9, color='red', label=\"Categories\")\n",
        "\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"PCA Dimension 1\")\n",
        "plt.ylabel(\"PCA Dimension 2\")\n",
        "plt.title(f\"2D PCA of Image and Text Embeddings (ViT-B/32)\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc886ec-5f17-455c-bb58-858b3ef930a1",
      "metadata": {
        "id": "bdc886ec-5f17-455c-bb58-858b3ef930a1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}