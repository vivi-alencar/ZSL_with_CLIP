{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivi-alencar/bachelor_thesis/blob/main/Training_Flickr_30k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jio0PKw_Ujkr",
      "metadata": {
        "id": "Jio0PKw_Ujkr"
      },
      "source": [
        "This version of the model uses an identity matrix as targets. Before, targets were based on the similarity between images and texts within the batch using softmax, which could weaken the model's ability to distinguish between matching and non-matching pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGX3uw3JwRWT",
      "metadata": {
        "id": "RGX3uw3JwRWT"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations\n",
        "!pip install timm\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db043925-40a1-497a-b2fa-ebb5350f07e3",
      "metadata": {
        "id": "db043925-40a1-497a-b2fa-ebb5350f07e3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6iqyELUyAprt",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6iqyELUyAprt"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "# Initialize the scaler for mixed precision\n",
        "scaler = torch.amp.GradScaler('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dVTAbi6ziHmC",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dVTAbi6ziHmC"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BGtFCAZhwuL2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BGtFCAZhwuL2"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle --upgrade\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXXXXXXX\"\n",
        "\n",
        "# Flickr 30k\n",
        "!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
        "!unzip flickr-image-dataset.zip\n",
        "dataset = \"30k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064691a8-02c8-4512-b60c-9255cc3bb372",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "064691a8-02c8-4512-b60c-9255cc3bb372"
      },
      "outputs": [],
      "source": [
        "# # Set up logging configuration\n",
        "# logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1b1332-fa17-4815-b4c3-4c8466decd84",
      "metadata": {
        "id": "7d1b1332-fa17-4815-b4c3-4c8466decd84"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o-ktE8JW-uC3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o-ktE8JW-uC3"
      },
      "outputs": [],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83479827-3370-4613-ba03-cef57acd2720",
      "metadata": {
        "id": "83479827-3370-4613-ba03-cef57acd2720"
      },
      "source": [
        "### Create pandas dataframe to add ids to images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "umO-vGT8l-J5",
      "metadata": {
        "id": "umO-vGT8l-J5"
      },
      "outputs": [],
      "source": [
        "# Read the captions file\n",
        "df = pd.read_csv(\"/content/flickr30k_images/results.csv\", delimiter=\"|\")\n",
        "df.columns = ['image', 'caption_number', 'caption']\n",
        "df['caption'] = df['caption'].str.lstrip()\n",
        "df['caption_number'] = df['caption_number'].str.lstrip()\n",
        "df.loc[19999, 'caption_number'] = \"4\"\n",
        "df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
        "\n",
        "# Add the 'id' column\n",
        "ids = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n",
        "df['id'] = ids\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv(\"captions.csv\", index=False)\n",
        "\n",
        "# Define the paths for images and captions\n",
        "image_path = \"/content/flickr30k_images/flickr30k_images\"\n",
        "captions_path = \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921e4393-3253-43a1-8ffb-3adb3efe364f",
      "metadata": {
        "id": "921e4393-3253-43a1-8ffb-3adb3efe364f"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9466c5df-b96f-4e31-b898-110419ddef72",
      "metadata": {
        "id": "9466c5df-b96f-4e31-b898-110419ddef72"
      },
      "outputs": [],
      "source": [
        "# Check if the file exists\n",
        "file_path = \"/content/captions.csv\"\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"File {file_path} exists.\")\n",
        "else:\n",
        "    print(f\"File {file_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a53357df-3a92-43d5-8f3e-6d5facbf2a47",
      "metadata": {
        "id": "a53357df-3a92-43d5-8f3e-6d5facbf2a47"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25511984-a69a-450f-8766-54dc9d69b72b",
      "metadata": {
        "id": "25511984-a69a-450f-8766-54dc9d69b72b"
      },
      "source": [
        "### CLASS: Store configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e2589e-da78-48a7-8858-bbc9c2e5d360",
      "metadata": {
        "id": "b0e2589e-da78-48a7-8858-bbc9c2e5d360"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    debug = False\n",
        "    #debug = True\n",
        "    image_path = image_path\n",
        "    captions_path = captions_path\n",
        "    batch_size = 32           # Number of samples processed in each batch during training/validation\n",
        "    num_workers = 2           # Number of subprocesses used for data loading\n",
        "    head_lr = 1e-3            # Learning rate for the projection heads (which map image and text embeddings to a common space)\n",
        "    image_encoder_lr = 1e-4   # Learning rate for the image encoder\n",
        "    text_encoder_lr = 1e-5    # Learning rate for the text encoder\n",
        "    weight_decay = 1e-3       # Regularization: add a penalty for large weights in the model\n",
        "\n",
        "    epochs = 20              # Number of epochs to train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = 'vit_base_patch32_224' # Image encoder\n",
        "    image_embedding = 768     # Embedding size for ViT\n",
        "    text_encoder_model = \"distilbert-base-uncased\" # Text encoder\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"distilbert-base-uncased\"\n",
        "    max_length = 200\n",
        "\n",
        "    pretrained = True        # for both image encoder and text encoder\n",
        "    trainable = True         # for both image encoder and text encoder\n",
        "    temperature = 1.2        # Used in the softmax function to control the sharpness of the probability distribution in contrastive learning\n",
        "\n",
        "    size = 224               # Image size\n",
        "\n",
        "    # For projection head:\n",
        "    num_projection_layers = 1 # Layers in the projection head\n",
        "    projection_dim = 256      # Size of the output embedding produced by the projection head\n",
        "    dropout = 0.1             # Regularization: randomly drop some neurons during training to prevent overfitting.\n",
        "\n",
        "    # Early stopping patience for validation loss improvement\n",
        "    early_stopping_patience = 5 # After N epochs without improvement, stop training\n",
        "\n",
        "    # ReduceLROnPlateau scheduler settings\n",
        "    lr_scheduler_patience = 2  # Number of epochs to wait before reducing the LR\n",
        "    lr_scheduler_factor = 0.8  # Factor to reduce LR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc31678-d522-4d2c-833e-0f52fe6a4aa8",
      "metadata": {
        "id": "2fc31678-d522-4d2c-833e-0f52fe6a4aa8"
      },
      "source": [
        "### CLASS: Track and compute the running average of a metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c8b996-25e4-4383-a0f1-bcb7dc3d1c4c",
      "metadata": {
        "id": "f1c8b996-25e4-4383-a0f1-bcb7dc3d1c4c"
      },
      "outputs": [],
      "source": [
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self): # This method sets all the internal counters (avg, sum, count) to zero, preparing the meter for a fresh run.\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1): # Updates the sum, count, and recalculates the average (avg) whenever a new value (val) is provided.\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self): #  string representation method that formats and returns the average value as a string. Handy for printing/logging purposes.\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01f10f0-98c7-41d0-8d01-03ec92d22bc7",
      "metadata": {
        "id": "b01f10f0-98c7-41d0-8d01-03ec92d22bc7"
      },
      "source": [
        "### FUNCTION: Retrieve the current learning rate from the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2cd5a3e-c0b1-485f-a2fc-8efebd3164ad",
      "metadata": {
        "id": "c2cd5a3e-c0b1-485f-a2fc-8efebd3164ad"
      },
      "outputs": [],
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9c77c7-d37e-4873-b01f-ad94b130658a",
      "metadata": {
        "id": "3f9c77c7-d37e-4873-b01f-ad94b130658a"
      },
      "source": [
        "### CLASS: Dataset class to load image-caption pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9b89b5-1421-492e-9951-f6a2222e8787",
      "metadata": {
        "id": "de9b89b5-1421-492e-9951-f6a2222e8787"
      },
      "outputs": [],
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset): #class inherits from torch.utils.data.Dataset\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms): # initialization method for the class\n",
        "        # image_filenames: list of image file names (i.e., paths to the images you want to load).\n",
        "        # captions: list of corresponding text captions for the images.\n",
        "        # tokenizer: tokenizer used to convert the captions into a format suitable for the model.\n",
        "        # transforms: set of image transformations applied to the images before feeding them into the model.\n",
        "\n",
        "        # Stores the list of image filenames\n",
        "        self.image_filenames = image_filenames\n",
        "\n",
        "        # Convert the captions into a list and stores them\n",
        "        self.captions = list(captions)\n",
        "\n",
        "        # Use tokenizer to convert captions into a dictionary of tokenized representations,\n",
        "        # with padding and truncation applied to match the desired maximum sequence length (CFG.max_length).\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "\n",
        "        # Store the image transformation functions for later use when loading the images\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx): # Define how to retrieve an individual sample (image and caption) from the dataset based on the provided index\n",
        "        # Loop through each tokenized item in encoded_captions (e.g., input_ids, attention_mask)\n",
        "        # and convert into PyTorch tensors, grabbing the specific tokenized caption at index idx.\n",
        "        # This creates the item dictionary where each key is a tokenized caption component, and each value is a tensor.\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "\n",
        "        # Load image corresponding to the index idx using PIL:\n",
        "        img_path = f\"{CFG.image_path}/{self.image_filenames[idx]}\"\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Open image and convert to RGB format\n",
        "\n",
        "        # Convert the PIL image to a NumPy array (some transformations expect NumPy arrays)\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Apply transformations (assuming the transforms expect a NumPy array)\n",
        "        image = self.transforms(image=image)['image']\n",
        "\n",
        "        # Convert the transformed image into a PyTorch tensor.\n",
        "        # Permute dimensions from (H, W, C) (height, width, channels) to (C, H, W) - PyTorch format for image tensors.\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "\n",
        "        # Add original (non-tokenized) caption to the item dictionary\n",
        "        item['caption'] = self.captions[idx]\n",
        "\n",
        "        # Return item dictionary, which now contains tokenized caption + transformed image data + original caption\n",
        "        return item\n",
        "\n",
        "    def __len__(self): # Method to return total number of samples in the dataset (length of the list of captions)\n",
        "        return len(self.captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9337329e-c2e7-409f-8bcf-9c56b4bc65dc",
      "metadata": {
        "id": "9337329e-c2e7-409f-8bcf-9c56b4bc65dc"
      },
      "source": [
        "### FUNCTION: Return a set of image transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228b522b-5d7c-4e98-b9e2-ac2b36e9f6c4",
      "metadata": {
        "id": "228b522b-5d7c-4e98-b9e2-ac2b36e9f6c4"
      },
      "outputs": [],
      "source": [
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\": # returns a set of transformations specifically tailored for training images\n",
        "        return A.Compose( #sequential transformations\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True), # Resize to CFG.size x CFG.size\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True), # Scale pixel values from [0, 255] to [0, 1]\n",
        "            ]\n",
        "        )\n",
        "    else: # Non-train mode\n",
        "        return A.Compose( # Same as above. Validation and testing require the same resizing/normalization\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cef12e-6ece-43c9-b87f-524f1faa1a5d",
      "metadata": {
        "id": "89cef12e-6ece-43c9-b87f-524f1faa1a5d"
      },
      "source": [
        "### CLASS: Image encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c5485e-2a50-4aa8-be4e-41008d8ecb6e",
      "metadata": {
        "id": "34c5485e-2a50-4aa8-be4e-41008d8ecb6e"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module): # Class inherits from nn.Module from PyTorch\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        #model_name: model architecture for image encoding\n",
        "        #pretrained: whether to use pre-trained version of the model\n",
        "        #trainable: determine whether the model's parameters should be trainable (True: updated during training, False:frozen)\n",
        "\n",
        "        # Call constructor of the parent class nn.Module (required when overriding the __init__ method in a subclass)\n",
        "        super().__init__()\n",
        "\n",
        "        # Create model with timm library\n",
        "        # With num_classes = 0, output classification layer is removed (model outputs a fixed-size feature vector (embedding) instead of class predictions.\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "\n",
        "        # Loop iterates through all model parameters and sets the requires_grad attribute based on the value of trainable\n",
        "        # requires_grad=True parameters are trainable/will be updated during backpropagation\n",
        "        # requires_grad=False: parameters are frozen/will not be updated during training.\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x): # Method for Forward pass of the model (how the input data flows through the network)\n",
        "        # x: input tensor (image or a batch of images)\n",
        "        return self.model(x) # pass x through the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86116af9-2489-40aa-82aa-a89341e0a8f3",
      "metadata": {
        "id": "86116af9-2489-40aa-82aa-a89341e0a8f3"
      },
      "source": [
        "### CLASS: Text encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ccb533-2df5-4564-aa1b-dff124776a54",
      "metadata": {
        "id": "e9ccb533-2df5-4564-aa1b-dff124776a54"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module): # Inherits from nn.Module\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        # model_name: name of the DistilBERT model to use.\n",
        "        # pretrained: whether to use a pre-trained version of DistilBERT. True: model is loaded with pre-trained weights. False: model is initialized with random weights.\n",
        "        # trainable: whether the DistilBERT model's parameters should be trainable. True: model will be fine-tuned during training. False: parameters are frozen.\n",
        "\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        # Set requires_grad attribute for all parameters in the DistilBERT model based on the trainable flag.\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # In BERT-based models (including DistilBERT), the [CLS] token is a special token that is added at the beginning of each input sequence.\n",
        "        # The hidden representation of this token is often used as the embedding for the entire sentence or sequence,\n",
        "        # as it is designed to represent the full meaning of the input.\n",
        "        self.target_token_idx = 0 # embedding for the [CLS] token (at position 0 in the sequence) will be used as the output embedding for the text sequence.\n",
        "\n",
        "    def forward(self, input_ids, attention_mask): # How the model processes input data.\n",
        "        # input_ids: tokenized input text sequences, represented as integers (tokens) that correspond to words or subwords.\n",
        "        # Each sequence starts with the [CLS] token.\n",
        "        # attention_mask: indicates which tokens are actual tokens and which are padding\n",
        "        # (in cases where sequences have different lengths). It allows the model to ignore the padding tokens during processing.\n",
        "\n",
        "        # Pass input ids and attention masks to DistilBERT, which returns object with various hidden states\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract last hidden state, which contains final hidden layer representations for all tokens in the sequence\n",
        "        last_hidden_state = output.last_hidden_state # Tensor of shape (batch_size, sequence_length, hidden_size).\n",
        "\n",
        "        # Extract the hidden state corresponding to the [CLS] token (which is at index 0)\n",
        "        return last_hidden_state[:, self.target_token_idx, :] # the fixed-size embedding representing the entire input sentence or sequence (tensor of shape (batch_size, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df202e0-1b0b-4d72-bba0-782118876460",
      "metadata": {
        "id": "2df202e0-1b0b-4d72-bba0-782118876460"
      },
      "source": [
        "### CLASS: Projection head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4985fb49-8e5c-4844-8c63-e677c7e9ffb2",
      "metadata": {
        "id": "4985fb49-8e5c-4844-8c63-e677c7e9ffb2"
      },
      "outputs": [],
      "source": [
        "# This class defines a projection head, which is responsible for mapping high-dimensional input embeddings into a lower-dimensional space\n",
        "# (often used before applying a loss function, like contrastive loss).\n",
        "class ProjectionHead(nn.Module): #inherits from nn.Module\n",
        "    def __init__( # Constructor\n",
        "        self,\n",
        "        embedding_dim, # the size of the input embeddings (dimensionality of the embeddings produced by the image encoder or text encoder)\n",
        "        projection_dim=CFG.projection_dim, # dimensionality of the space to which the embeddings will be projected\n",
        "        dropout=CFG.dropout # prevent overfitting\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim) # Fully connected layer, projects from embedding_dim to projection_dim\n",
        "        self.gelu = nn.GELU() # nonlinear activation function. Allows network to model complex relationships between the input and output.\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim) # Fully connected layer, projects intermediate space back to the same dimensionality\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout randomly sets some elements of the tensor to zero during training, helping the network generalize better by preventing overfitting.\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim) # helps smooth out the values and avoid exploding/vanishing gradients during training.\n",
        "\n",
        "    def forward(self, x): #  defines how the input embeddings x flow through the layers of the projection head.\n",
        "        projected = self.projection(x) #  input x is projected to a lower-dimensional space (projection_dim) using the first fully connected layer.\n",
        "        x = self.gelu(projected) # projected output is passed through the GELU activation function to introduce non-linearity.\n",
        "        x = self.fc(x) #  fully connected layer is applied to the output of GELU, maintaining the same dimensionality (projection_dim).\n",
        "        x = self.dropout(x) # Dropout randomly sets some elements to zero. Helps with regularization and reduces overfitting.\n",
        "        x = x + projected # Residual connection is added: original projection (projected) is added to the output of the fully connected layer.\n",
        "                          # Stabilizes training and prevents gradient issues. Allows model to learn differences from the original input rather than completely transforming it.\n",
        "        x = self.layer_norm(x) #  output is normalized across the feature dimension using layer normalization to ensure smooth training\n",
        "        return x # returns the final transformed embedding, which is now in the projection_dim space and ready for downstream tasks (e.g., contrastive loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbec9fc-f0d5-4b09-b638-3a70773860f9",
      "metadata": {
        "id": "8cbec9fc-f0d5-4b09-b638-3a70773860f9"
      },
      "source": [
        "### CLASS: Clip Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdffcd1-a2be-40b4-9b21-1f62db17787a",
      "metadata": {
        "id": "2bdffcd1-a2be-40b4-9b21-1f62db17787a"
      },
      "outputs": [],
      "source": [
        "# Encode both images and text into a shared embedding space, then calculate the contrastive loss between the two.\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__( # Constructor\n",
        "        self,\n",
        "        temperature=CFG.temperature, # scalar value used to scale the logits (similarities) before applying softmax.\n",
        "                                     #It controls the sharpness of the output distribution.\n",
        "                                     #A lower temperature sharpens the distribution, making high similarities more dominant.\n",
        "        image_embedding=CFG.image_embedding, # Dimensionality of the image embeddings (i.e., the size of the vector produced by the ImageEncoder).\n",
        "        text_embedding=CFG.text_embedding, # Dimensionality of the text embeddings (i.e., the size of the vector produced by the TextEncoder).\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder() # generates feature embeddings for the input images.\n",
        "        self.text_encoder = TextEncoder() # generates feature embeddings for the input text.\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding) # takes output from img encoder and projects into low-dim space for contrastive learning\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding) # similarly for text\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Input to model: batch, a dictionary containint image, input ids and attention mask\n",
        "        image_features = self.image_encoder(batch[\"image\"]) # batch of input images.\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"] # tokenized text sequences and corresponding attention masks.\n",
        "        )\n",
        "\n",
        "        # Getting Image and Text Embeddings (with same dimension)\n",
        "        image_embeddings = self.image_projection(image_features) # project image features.\n",
        "        text_embeddings = self.text_projection(text_features) # project text features.\n",
        "\n",
        "        # Calculating Constrastive Loss:\n",
        "        # 1. Compute similarity scores between the projected text embeddings and the image embeddings using matrix multiplication (@ operator).\n",
        "        # The dot product between every text embedding and every image embedding is computed to create the logits matrix.\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature # similarity matrix: row = text, column = image.\n",
        "\n",
        "        # 2. Identity matrix as target for cross-entropy loss\n",
        "        batch_size = logits.shape[0]  # This should be the same for both text and image logits\n",
        "        targets = torch.eye(batch_size, device=logits.device)  # Ensure batch size consistency\n",
        "\n",
        "\n",
        "        # 3. Calculate loss using cross-entropy\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "\n",
        "        # 4. Final loss as the average of the image and text losses, ensuring symmetry between the two modalities.\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "\n",
        "        return loss.mean() # The mean loss is returned for backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e3a566-37e3-424e-85ea-f96e0cd2de81",
      "metadata": {
        "id": "74e3a566-37e3-424e-85ea-f96e0cd2de81"
      },
      "source": [
        "### FUNCTION: Custom cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c475c43d-0a82-421e-8418-6033cbfb7523",
      "metadata": {
        "id": "c475c43d-0a82-421e-8418-6033cbfb7523"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    # preds: Predicted values (logits) from the model.\n",
        "    #       These are unnormalized scores (typically before applying softmax) that represent the model's confidence in each class.\n",
        "    # targets: Identity matrix representing the ground truth (matching pairs).\n",
        "    # reduction='none': Specifies how to reduce (aggregate) the loss across the batch.\n",
        "    #                   none: No reduction, the per-sample loss is returned.\n",
        "    #                   mean: The loss is averaged across all samples in the batch.\n",
        "\n",
        "    # Create a LogSoftmax layer that applies the log of the softmax function along the last dimension (dim=-1)\n",
        "    # Softmax converts logits (unnormalized model predictions) into probabilities.\n",
        "    # LogSoftmax gives the natural logarithm of these probabilities, which is useful for computing log-likelihood-based losses.\n",
        "    # It is numerically more stable than computing softmax followed by a logarithm\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    # Calculate the loss\n",
        "    # Since the targets will be an identity matrix, this computes the negative log likelihood\n",
        "    # for the matching pairs (diagonal entries) and considers non-matching pairs (off-diagonal\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "\n",
        "    # Handling Reduction\n",
        "    if reduction == \"none\": # function returns the individual loss values for each sample in the batch (i.e., no aggregation is applied).\n",
        "        return loss\n",
        "    elif reduction == \"mean\": # function returns the mean of the individual loss values across the batch.\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684d55dd-4784-46ff-9977-1c5e12ed0f0f",
      "metadata": {
        "id": "684d55dd-4784-46ff-9977-1c5e12ed0f0f"
      },
      "source": [
        "### FUNCTION: Create Splits (80% Training, 20% Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d7bdef-c14b-4559-8ea4-d50a039d8365",
      "metadata": {
        "id": "83d7bdef-c14b-4559-8ea4-d50a039d8365"
      },
      "outputs": [],
      "source": [
        "def make_train_valid_dfs():\n",
        "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\") # read CSV file containing captions and image IDs into a Pandas DataFrame.\n",
        "\n",
        "    # Find maximum value in the id column (which represents the highest image ID) and add 1 to it (helps establish the number of unique images)\n",
        "    # If CFG.debug is set to True, the max_id is capped at 100 (indicating only 100 samples will be used for debugging).\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "\n",
        "    # Create array of image IDs ranging from 0 to max_id-1. These IDs represent all the unique image IDs in the dataset.\n",
        "    image_ids = np.arange(0, max_id)\n",
        "\n",
        "    # Set random seed\n",
        "    np.random.seed(42) # ensures that the random splitting of data is reproducible\n",
        "\n",
        "    # Select Validation IDs:\n",
        "    # a) np.random.choice() selects a random subset of image IDs for the validation set.\n",
        "    # b) size=int(0.2 * len(image_ids)): The size of the validation set is 20% of the total image IDs (calculated as 0.2 * len(image_ids)).\n",
        "    # c) replace=False: This ensures that the same image ID cannot be selected more than once.\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "\n",
        "    # Create a list of image IDs for the training set, containing all the IDs that are not in the validation set.\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids] # uses a list comprehension to filter out IDs that are in the validation set.\n",
        "\n",
        "    # Create training and validation DataFrames:\n",
        "    # a) dataframe[\"id\"].isin(train_ids): filters the rows of the original DataFrame where the image id is in the train_ids list.\n",
        "    # b) (valid_dataframe) is created by filtering the rows where the id is in the valid_ids list.\n",
        "    # c) reset_index(drop=True): resets the index of the resulting DataFrames,\n",
        "                                 # ensuring that the new DataFrames have a clean index starting from 0,\n",
        "                                 # and drop=True ensures that the old index is not added as a column.\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True) # create the training DataFrame.\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "    # # Log sizes of train and validation sets\n",
        "    # logging.info(f'Training set size: {len(train_dataframe)}')\n",
        "    # logging.info(f'Validation set size: {len(valid_dataframe)}')\n",
        "\n",
        "    return train_dataframe, valid_dataframe # returns two DataFrames: one for training and one for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639521ef-b8a6-4456-befa-26bc3b18077a",
      "metadata": {
        "id": "639521ef-b8a6-4456-befa-26bc3b18077a"
      },
      "source": [
        "### FUNCTION: Create data loaders for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7ee02a-d0ca-4d60-a0e3-ad26c893f5a0",
      "metadata": {
        "id": "bb7ee02a-d0ca-4d60-a0e3-ad26c893f5a0"
      },
      "outputs": [],
      "source": [
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    # dataframe: Pandas DataFrame containing image paths and captions\n",
        "    # tokenizer: tokenizer\n",
        "    # mode: whether the DataLoader is being created for training or validation. Determines the behavior for data augmentation and shuffling.\n",
        "\n",
        "    # Call function that returns different image transformations based on whether the model is in:\n",
        "    #     training mode\n",
        "    #     validation mode\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values, # pass the array of image file paths from the DataFrame.\n",
        "        dataframe[\"caption\"].values, # pass the array of captions from the DataFrame.\n",
        "        tokenizer=tokenizer, # to tokenize captions\n",
        "        transforms=transforms, # image transformations, specific to the mode (train/validation), are applied to the images in the dataset.\n",
        "    )\n",
        "\n",
        "    # Create a PyTorch DataLoader\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, # dataset object created above\n",
        "        batch_size=CFG.batch_size, # batch size for loading the data\n",
        "        num_workers=CFG.num_workers, # number of worker processes used to load the data in parallel. More workers = faster data loading (depends on the system’s hardware).\n",
        "        shuffle=True if mode == \"train\" else False, # Shuffling is important during training to ensure that the model doesn't learn the order of the data\n",
        "    )\n",
        "\n",
        "    # # Log some information about the dataloader\n",
        "    # logging.info(f'{mode.capitalize()} DataLoader created with {len(dataset)} samples, batch size {CFG.batch_size}, and {CFG.num_workers} workers.')\n",
        "\n",
        "    return dataloader # returns the DataLoader, which can then be used in the training/validation loop to load batches of images and captions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0af2ed4-7609-4a6e-ba7e-fd72aa3299da",
      "metadata": {
        "id": "d0af2ed4-7609-4a6e-ba7e-fd72aa3299da"
      },
      "source": [
        "### FUNCTION: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb13576-a5db-4749-a077-9e61a42923e7",
      "metadata": {
        "id": "fbb13576-a5db-4749-a077-9e61a42923e7"
      },
      "outputs": [],
      "source": [
        "# This function processes a single epoch of training,\n",
        "# updating the model weights using the optimizer and adjusting the learning rate using the learning rate scheduler.\n",
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    # model: The neural network model being trained\n",
        "    # train_loader: The DataLoader for the training dataset, which yields batches of data during training.\n",
        "    # optimizer: The optimizer responsible for updating the model's weights.\n",
        "    # lr_scheduler: A learning rate scheduler that adjusts the learning rate during training.\n",
        "    # step: Determines when the learning rate scheduler should be updated, either at each batch or at the end of each epoch.\n",
        "\n",
        "    # Initialize Loss Tracking\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    # Initialize Progress Bar (tqdm)\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    # Main training loop\n",
        "    for batch in tqdm_object: # iterate over each batch of data in the train_loader.\n",
        "\n",
        "        # 1. Move data to device\n",
        "        # The if k != \"caption\" condition excludes the caption (text data), which is likely not needed by the model directly in this form.\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        # 2. Zero out the gradients of the previous batch before updating model weights.\n",
        "        # This ensures that gradients from earlier batches don’t accumulate.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3. Forward pass and loss calculation\n",
        "        #    Use autocast for mixed precision training (if enabled)\n",
        "        with autocast():\n",
        "            loss = model(batch)\n",
        "\n",
        "        # 4. Backward Pass (Compute Gradients) with Scaled Gradients\n",
        "        # This step calculates how much each parameter should be adjusted to minimize the loss.\n",
        "        scaler.scale(loss).backward() # compute the gradients of the loss with respect to the model's parameters using backpropagation.\n",
        "\n",
        "        # 5. Optimizer Step (Update Weights) with Scaled Gradients\n",
        "        #  This step updates the model’s parameters using the gradients that were computed in the previous step.\n",
        "        # The optimizer adjusts the parameters in the direction that minimizes the loss.\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # 6. Update Loss Tracking\n",
        "        count = batch[\"image\"].size(0) # gets the number of samples in the current batch\n",
        "        loss_meter.update(loss.item(), count) #  update the running average of the loss using the loss value for this batch (loss.item()) and the batch size (count).\n",
        "\n",
        "        # 7. Update Progress Bar\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
        "\n",
        "        # # 8. Logging loss and learning rate per batch\n",
        "        # logging.info(f\"Batch loss: {loss_meter.avg:.4f}, Learning rate: {get_lr(optimizer):.6f}\")\n",
        "\n",
        "    # # Log final loss at the end of the epoch\n",
        "    # logging.info(f\"End of epoch loss: {loss_meter.avg:.4f}\")\n",
        "\n",
        "    return loss_meter # contains the running average of the training loss over the entire epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bbaabb-d810-4792-a2fa-110670c82a23",
      "metadata": {
        "id": "58bbaabb-d810-4792-a2fa-110670c82a23"
      },
      "source": [
        "### FUNCTION: Handle the validation phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3f8800-52a5-41d1-88bf-5f305f6e776c",
      "metadata": {
        "id": "cd3f8800-52a5-41d1-88bf-5f305f6e776c"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, valid_loader):\n",
        "    # model: The neural network model being validated.\n",
        "    # valid_loader: A DataLoader object that yields batches of data from the validation set.\n",
        "    # The validation DataLoader is created in a similar way as the training DataLoader,\n",
        "    # but typically without data augmentation and without shuffling.\n",
        "\n",
        "    # Initialize Loss Tracking\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    # Initialize Progress Bar (tqdm)\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "\n",
        "        # Main Validation Loop\n",
        "        for batch in tqdm_object: #  iterate through each batch of validation data yielded by the valid_loader.\n",
        "\n",
        "            # 1. Move data to device\n",
        "            batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "            # 2. Forward Pass and Loss Calculation\n",
        "            # No backward pass or gradient updates are required here since this is validation.\n",
        "            loss = model(batch)\n",
        "\n",
        "            # 3. Update Loss Tracking\n",
        "            count = batch[\"image\"].size(0) # the number of images in the current batch;  used to properly weight the average loss calculation.\n",
        "            loss_meter.update(loss.item(), count) # The loss for this batch (loss.item()) and the batch size (count) are used to update the running average of the loss (AvgMeter).\n",
        "\n",
        "            # 4. Update Progress Bar\n",
        "            tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
        "\n",
        "    return loss_meter # contains the final average validation loss for the entire epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0be51a-27a9-484b-9dbb-6b105a9cccd5",
      "metadata": {
        "id": "4a0be51a-27a9-484b-9dbb-6b105a9cccd5"
      },
      "source": [
        "### Function to plot loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce781ab3-0679-4448-a238-2e3b1380d77e",
      "metadata": {
        "id": "ce781ab3-0679-4448-a238-2e3b1380d77e"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(train_losses, valid_losses):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.grid(True)  # Add grid for better readability\n",
        "    plt.savefig('loss_curves.png')  # Save the plot as a file (optional)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e231e255-c10d-4bc4-b4ab-a0dc232692aa",
      "metadata": {
        "id": "e231e255-c10d-4bc4-b4ab-a0dc232692aa"
      },
      "source": [
        "## FUNCTION: Save checkpoint to resume training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623ac3d1-60cf-4428-bcf4-d6fe68b6cd93",
      "metadata": {
        "id": "623ac3d1-60cf-4428-bcf4-d6fe68b6cd93"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, lr_scheduler, best_loss, file_path=\"checkpoint.pth\"):\n",
        "    # Create a dictionary containing all the necessary training information\n",
        "    state = {\n",
        "        'epoch': epoch,  # Save the current epoch\n",
        "        'model_state_dict': model.state_dict(),  # Save the model's parameters\n",
        "        'optimizer_state_dict': optimizer.state_dict(),  # Save the optimizer's state\n",
        "        'scheduler_state_dict': lr_scheduler.state_dict(),  # Save the scheduler's state\n",
        "        'best_loss': best_loss  # Save the best validation loss encountered so far\n",
        "    }\n",
        "\n",
        "    # Save the dictionary as a file using torch.save\n",
        "    torch.save(state, file_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ce3f9c-aa22-48d7-9504-8043f5df8144",
      "metadata": {
        "id": "a4ce3f9c-aa22-48d7-9504-8043f5df8144"
      },
      "source": [
        "## FUNCTION: Load saved checkpoint to resume training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f21e429-dc81-46a8-a642-ab11c0e5824f",
      "metadata": {
        "id": "3f21e429-dc81-46a8-a642-ab11c0e5824f"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(file_path, model, optimizer, lr_scheduler):\n",
        "    try:\n",
        "        checkpoint = torch.load(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file not found: {file_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load the saved model, optimizer, and scheduler states\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    # Return the epoch and best validation loss to resume training\n",
        "    return checkpoint['epoch'], checkpoint['best_loss']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c547d642-039f-4ec7-af2b-9d5536b67150",
      "metadata": {
        "id": "c547d642-039f-4ec7-af2b-9d5536b67150"
      },
      "source": [
        "### FUNCTION: Main (training and validation loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717ba1a5-dd63-49a6-b544-33860fbd81c1",
      "metadata": {
        "id": "717ba1a5-dd63-49a6-b544-33860fbd81c1"
      },
      "outputs": [],
      "source": [
        "def main(resume=False): # Set to TRUE if training was interrupted\n",
        "    #  Prepare Training and Validation DataFrames\n",
        "    train_df, valid_df = make_train_valid_dfs() #  split the dataset into training and validation sets. It returns two DataFrames\n",
        "\n",
        "    # Initialize Tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    # Build Data Loaders\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    # Initialize the Model\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "\n",
        "    # Set Up the Optimizer Parameters\n",
        "    # Different learning rates for different parts of the model\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr}, # specifies the parameters of the image encoder. No decay.\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr}, # sets the learning rate for the text encoder parameters. No decay.\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters() # groups the parameters of both the image and text projection heads. Decay.\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "\n",
        "    # Initialize the Optimizer\n",
        "    optimizer = torch.optim.AdamW(params)\n",
        "\n",
        "    # Initialize the Learning Rate Scheduler:\n",
        "    #    a) torch.optim.lr_scheduler.ReduceLROnPlateau: This learning rate scheduler reduces the learning rate when a metric\n",
        "    #       (in this case, validation loss) stops improving.\n",
        "    #    b) mode=\"min\": The scheduler monitors the minimum validation loss, meaning the learning rate will decrease when the loss plateaus or increases.\n",
        "    #    c) patience=CFG.patience: Number of epochs with no improvement after which the learning rate will be reduced.\n",
        "    #    d) factor=CFG.factor: The factor by which the learning rate will be reduced (e.g., if factor=0.1, the learning rate is multiplied by 0.1).\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.lr_scheduler_patience, factor=CFG.lr_scheduler_factor\n",
        "    )\n",
        "\n",
        "    # Initialize variables to track progress\n",
        "    best_loss = float('inf') # initialize best_loss to infinity so that any actual loss from validation can be considered better.\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Resume from checkpoint if needed\n",
        "    if resume:\n",
        "        # If resuming, load from the checkpoint file\n",
        "        checkpoint = torch.load(\"checkpoint.pth\")\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_loss = checkpoint['best_loss']\n",
        "        print(f\"Resuming from epoch {start_epoch+1} with best validation loss {best_loss:.4f}\")\n",
        "\n",
        "    #  Training Loop\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = CFG.early_stopping_patience  # From CFG\n",
        "\n",
        "    # Initialize lists to store loss values for plotting\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch: {epoch + 1}\")\n",
        "\n",
        "        #  Training Phase\n",
        "        model.train() # ensures that certain layers like dropout and batch normalization behave differently compared to evaluation mode.\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step=\"epoch\")\n",
        "        train_losses.append(train_loss.avg) # for plotting\n",
        "\n",
        "        #  Validation Phase\n",
        "        model.eval() # changes the behavior of certain layers like dropout and batch normalization.\n",
        "        with torch.no_grad(): # disables gradient computation during validation to reduce memory usage and speed up the process.\n",
        "            valid_loss = valid_epoch(model, valid_loader)\n",
        "        valid_losses.append(valid_loss.avg) # for plotting\n",
        "\n",
        "        ## DEBUG ##\n",
        "        #   Print the losses and learning rate for debugging purposes\n",
        "        print(f\"Training Loss: {train_loss.avg:.4f}, Validation Loss: {valid_loss.avg:.4f}\")\n",
        "        print(f\"Learning Rate: {get_lr(optimizer):.6f}\")\n",
        "\n",
        "        ## REMOVE IF NOT USED\n",
        "        # #  Log Metrics (Training Loss, Validation Loss)\n",
        "        # log_metrics(epoch + 1, train_loss.avg, valid_loss.avg)  # Log training and validation loss\n",
        "\n",
        "        #  Save the Best Model and Checkpoint\n",
        "        if valid_loss.avg < best_loss: # If the validation loss for this epoch is better (lower) than the previous best loss, save the model’s parameters.\n",
        "            best_loss = valid_loss.avg # update best_loss to the current epoch’s validation loss.\n",
        "            patience_counter = 0  # Reset patience if there's an improvement\n",
        "            torch.save(model.state_dict(), \"best30k07.pt\") # save the model’s parameters to a file named best.pt when the best model is found.\n",
        "            print(\"Saved Best Model!\")\n",
        "        else:\n",
        "            patience_counter += 1  # Increment patience counter if no improvement\n",
        "\n",
        "        # Save checkpoint at the end of each epoch\n",
        "        save_checkpoint(epoch, model, optimizer, lr_scheduler, best_loss)\n",
        "\n",
        "        #  Early Stopping Check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement.\")\n",
        "            break\n",
        "\n",
        "        #  Learning Rate Adjustment:\n",
        "        #       If the validation loss stops improving, the learning rate is reduced according to the learning rate scheduler settings.\n",
        "        lr_scheduler.step(valid_loss.avg)\n",
        "\n",
        "    #  Plot the loss curves after training is completed\n",
        "    plot_loss_curves(train_losses, valid_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2922b713-4225-4091-b779-53c71e7fb24c",
      "metadata": {
        "id": "2922b713-4225-4091-b779-53c71e7fb24c"
      },
      "outputs": [],
      "source": [
        " main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}