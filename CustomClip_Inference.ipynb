{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivi-alencar/bachelor_thesis/blob/main/CustomClip_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a76e99-8d70-491f-ab66-aa65bde345f0",
      "metadata": {
        "id": "11a76e99-8d70-491f-ab66-aa65bde345f0"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97256d8b-df5d-4400-9f36-f8f002e2ea0f",
      "metadata": {
        "id": "97256d8b-df5d-4400-9f36-f8f002e2ea0f"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae71311-59b1-46d0-be21-678748d3c557",
      "metadata": {
        "id": "3ae71311-59b1-46d0-be21-678748d3c557"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1aa7c1-d592-4cfb-8253-9a3655d02d00",
      "metadata": {
        "id": "ef1aa7c1-d592-4cfb-8253-9a3655d02d00"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0983d5e9-81d1-484c-95e7-59dd8a39f9e7",
      "metadata": {
        "id": "0983d5e9-81d1-484c-95e7-59dd8a39f9e7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872f96c9-ddd3-4ab4-a165-48322cd688f1",
      "metadata": {
        "id": "872f96c9-ddd3-4ab4-a165-48322cd688f1"
      },
      "outputs": [],
      "source": [
        "image_path = \"path to coco images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e9290d-d093-42fe-a223-095190439d90",
      "metadata": {
        "id": "85e9290d-d093-42fe-a223-095190439d90"
      },
      "outputs": [],
      "source": [
        "model_path = \"path to trained model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2a21ce-ad9f-4a63-86b9-60e4a33c7f0b",
      "metadata": {
        "id": "1c2a21ce-ad9f-4a63-86b9-60e4a33c7f0b"
      },
      "outputs": [],
      "source": [
        "model_name = \"name of saved model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a0d3af-bf01-4768-9fb6-bf150a4dd01c",
      "metadata": {
        "id": "f8a0d3af-bf01-4768-9fb6-bf150a4dd01c"
      },
      "outputs": [],
      "source": [
        "### CLASS: Store configurations\n",
        "class CFG:\n",
        "    debug = False\n",
        "    #debug = True\n",
        "    image_path = image_path\n",
        "    batch_size = 32           # Number of samples processed in each batch during training/validation\n",
        "    num_workers = 2           # Number of subprocesses used for data loading\n",
        "    head_lr = 5e-4            # Learning rate for the projection heads (which map image and text embeddings to a common space)\n",
        "    image_encoder_lr = 5e-5   # Learning rate for the image encoder\n",
        "    text_encoder_lr = 1e-5    # Learning rate for the text encoder\n",
        "    weight_decay = 1e-3       # Regularization: add a penalty for large weights in the model\n",
        "\n",
        "    epochs = 30              # Number of epochs to train the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = 'vit_base_patch32_224' # Image encoder\n",
        "    image_embedding = 768     # Embedding size for ViT after processing the image\n",
        "    text_encoder_model = \"distilbert-base-uncased\" # Text encoder\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"distilbert-base-uncased\"\n",
        "    max_length = 200\n",
        "\n",
        "    pretrained = True        # for both image encoder and text encoder\n",
        "    trainable = True         # for both image encoder and text encoder\n",
        "    temperature = 1.0        # Used in the softmax function to control the sharpness of the probability distribution in contrastive learning\n",
        "\n",
        "    size = 224               # Input image size\n",
        "\n",
        "    # For projection head:\n",
        "    num_projection_layers = 1 # Layers in the projection head\n",
        "    projection_dim = 256      # Size of the output embedding produced by the projection head\n",
        "    dropout = 0.1             # Regularization: randomly drop some neurons during training to prevent overfitting. Recommended: 0.1\n",
        "\n",
        "    # Early stopping patience for validation loss improvement\n",
        "    early_stopping_patience = 7 # After N epochs without improvement, stop training\n",
        "\n",
        "    # ReduceLROnPlateau scheduler settings\n",
        "    lr_scheduler_patience = 3  # Number of epochs to wait before reducing the LR\n",
        "    lr_scheduler_factor = 0.8  # Factor to reduce LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd87f918-6179-4775-bdc0-f9e77c26ce44",
      "metadata": {
        "id": "fd87f918-6179-4775-bdc0-f9e77c26ce44"
      },
      "outputs": [],
      "source": [
        "### CLASS: Track and compute the running average of a metric\"\"\"\n",
        "\n",
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self): # This method sets all the internal counters (avg, sum, count) to zero, preparing the meter for a fresh run.\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1): # Updates the sum, count, and recalculates the average (avg) whenever a new value (val) is provided.\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self): #  string representation method that formats and returns the average value as a string. Handy for printing/logging purposes.\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926d471a-2f3a-496d-a7a3-604a8db94b88",
      "metadata": {
        "id": "926d471a-2f3a-496d-a7a3-604a8db94b88"
      },
      "outputs": [],
      "source": [
        "### FUNCTION: Retrieve the current learning rate from the optimizer\"\"\"\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8485fce2-c305-4e15-af09-880ede114b5d",
      "metadata": {
        "id": "8485fce2-c305-4e15-af09-880ede114b5d"
      },
      "outputs": [],
      "source": [
        "### CLASS: Dataset class to load image-class pairs\"\"\"\n",
        "\n",
        "class CLIPDataset(torch.utils.data.Dataset): #class inherits from torch.utils.data.Dataset\n",
        "    def __init__(self, image_filenames, classes, tokenizer=None, transforms=None): # initialization method for the class\n",
        "        # image_filenames: list of image file names (i.e., paths to the images you want to load).\n",
        "        # classes: list of corresponding classes for the images.\n",
        "        # tokenizer: tokenizer used to convert the classes into a format suitable for the model.\n",
        "        # transforms: set of image transformations applied to the images before feeding them into the model.\n",
        "\n",
        "        # Stores the list of image filenames\n",
        "        self.image_filenames = image_filenames\n",
        "\n",
        "        # Convert the classes into a list and stores them\n",
        "        self.classes = list(classes)\n",
        "\n",
        "        # DEBUG\n",
        "        print(f\"Initializing CLIPDataset with {len(self.image_filenames)} images.\")\n",
        "\n",
        "        if tokenizer is not None:\n",
        "            try:\n",
        "                self.encoded_classes = tokenizer(\n",
        "                    list(classes), padding=True, truncation=True, max_length=CFG.max_length, return_tensors='pt'\n",
        "                )\n",
        "                print(\"Classes tokenized successfully.\")\n",
        "            except Exception as e:\n",
        "                print(\"Error during tokenization:\", e)\n",
        "                self.encoded_classes = None\n",
        "        else:\n",
        "            self.encoded_classes = None\n",
        "            print(\"No tokenizer provided; skipping class encoding.\")\n",
        "\n",
        "        # # Use tokenizer to convert classes into a dictionary of tokenized representations,\n",
        "        # # with padding and truncation applied to match the desired maximum sequence length (CFG.max_length).\n",
        "        # self.encoded_classes = tokenizer(\n",
        "        #     list(classes), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        # )\n",
        "\n",
        "        # Store the image transformation functions for later use when loading the images\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx): # Define how to retrieve an individual sample (image and class) from the dataset based on the provided index\n",
        "        # Loop through each tokenized item in encoded_classes (e.g., input_ids, attention_mask)\n",
        "        # and convert into PyTorch tensors, grabbing the specific tokenized class at index idx.\n",
        "        # This creates the item dictionary where each key is a tokenized class component, and each value is a tensor.\n",
        "        # item = {\n",
        "        #     key: torch.tensor(values[idx])\n",
        "        #     for key, values in self.encoded_classes.items()\n",
        "        # }\n",
        "        item = {}\n",
        "\n",
        "        if self.encoded_classes is not None:\n",
        "            item = {\n",
        "                key: torch.tensor(values[idx])\n",
        "                for key, values in self.encoded_classes.items()\n",
        "            }\n",
        "            #print(f\"Encoded class for index {idx}: {item}\")\n",
        "\n",
        "        # Load image corresponding to the index idx using PIL:\n",
        "        img_path = f\"{CFG.image_path}/{self.image_filenames[idx]}\"\n",
        "        #print(f\"Loading image from path: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Open image and convert to RGB format\n",
        "\n",
        "        # Convert the PIL image to a NumPy array (some transformations expect NumPy arrays)\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['class'] = self.classes[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self): # Method to return total number of samples in the dataset (length of the list of classes)\n",
        "        return len(self.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b045e5-d482-4c46-b071-0999f22da306",
      "metadata": {
        "id": "c9b045e5-d482-4c46-b071-0999f22da306"
      },
      "outputs": [],
      "source": [
        "### CLASS: Image encoder\"\"\"\n",
        "\n",
        "class ImageEncoder(nn.Module): # Class inherits from nn.Module from PyTorch\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        #model_name: model architecture for image encoding\n",
        "        #pretrained: whether to use pre-trained version of the model\n",
        "        #trainable: determine whether the model's parameters should be trainable (True: updated during training, False:frozen)\n",
        "\n",
        "        # Call constructor of the parent class nn.Module (required when overriding the __init__ method in a subclass)\n",
        "        super().__init__()\n",
        "\n",
        "        # Create model with timm library\n",
        "        # With num_classes = 0, output classification layer is removed (model outputs a fixed-size feature vector (embedding) instead of class predictions.\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "\n",
        "        # Loop iterates through all model parameters and sets the requires_grad attribute based on the value of trainable\n",
        "        # requires_grad=True parameters are trainable/will be updated during backpropagation\n",
        "        # requires_grad=False: parameters are frozen/will not be updated during training.\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x): # Method for Forward pass of the model (how the input data flows through the network)\n",
        "        # x: input tensor (image or a batch of images)\n",
        "        return self.model(x) # pass x through the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a7feda-570b-466e-a3bd-f9323846d37e",
      "metadata": {
        "id": "61a7feda-570b-466e-a3bd-f9323846d37e"
      },
      "outputs": [],
      "source": [
        "### FUNCTION: Return a set of image transformations\"\"\"\n",
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\": # returns a set of transformations specifically tailored for training images\n",
        "        return A.Compose( #sequential transformations\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True), # Resize to CFG.size x CFG.size\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True), # Scale pixel values from [0, 255] to [0, 1]\n",
        "            ]\n",
        "        )\n",
        "    else: # Non-train mode\n",
        "        return A.Compose( # Same as above. Validation and testing require the same resizing/normalization\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e157ca-37c5-447e-84aa-a181a09e4558",
      "metadata": {
        "id": "03e157ca-37c5-447e-84aa-a181a09e4558"
      },
      "outputs": [],
      "source": [
        "### CLASS: Text encoder\"\"\"\n",
        "\n",
        "class TextEncoder(nn.Module): # Inherits from nn.Module\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        # model_name: name of the DistilBERT model to use.\n",
        "        # pretrained: whether to use a pre-trained version of DistilBERT. True: model is loaded with pre-trained weights. False: model is initialized with random weights.\n",
        "        # trainable: whether the DistilBERT model's parameters should be trainable. True: model will be fine-tuned during training. False: parameters are frozen.\n",
        "\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        # Set requires_grad attribute for all parameters in the DistilBERT model based on the trainable flag.\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # In BERT-based models (including DistilBERT), the [CLS] token is a special token that is added at the beginning of each input sequence.\n",
        "        # The hidden representation of this token is often used as the embedding for the entire sentence or sequence,\n",
        "        # as it is designed to represent the full meaning of the input.\n",
        "        self.target_token_idx = 0 # embedding for the [CLS] token (at position 0 in the sequence) will be used as the output embedding for the text sequence.\n",
        "\n",
        "    def forward(self, input_ids, attention_mask): # How the model processes input data.\n",
        "        # input_ids: tokenized input text sequences, represented as integers (tokens) that correspond to words or subwords.\n",
        "        # Each sequence starts with the [CLS] token.\n",
        "        # attention_mask: indicates which tokens are actual tokens and which are padding\n",
        "        # (in cases where sequences have different lengths). It allows the model to ignore the padding tokens during processing.\n",
        "\n",
        "        # Pass input ids and attention masks to DistilBERT, which returns object with various hidden states\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract last hidden state, which contains final hidden layer representations for all tokens in the sequence\n",
        "        last_hidden_state = output.last_hidden_state # Tensor of shape (batch_size, sequence_length, hidden_size).\n",
        "\n",
        "        # Extract the hidden state corresponding to the [CLS] token (which is at index 0)\n",
        "        return last_hidden_state[:, self.target_token_idx, :] # the fixed-size embedding representing the entire input sentence or sequence (tensor of shape (batch_size, hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41781a1-d0e6-4e24-b567-d9aafc72093e",
      "metadata": {
        "id": "e41781a1-d0e6-4e24-b567-d9aafc72093e"
      },
      "outputs": [],
      "source": [
        "### CLASS: Projection head\"\"\"\n",
        "\n",
        "# This class defines a projection head, which is responsible for mapping high-dimensional input embeddings into a lower-dimensional space\n",
        "# (often used before applying a loss function, like contrastive loss).\n",
        "class ProjectionHead(nn.Module): #inherits from nn.Module\n",
        "    def __init__( # Constructor\n",
        "        self,\n",
        "        embedding_dim, # the size of the input embeddings (dimensionality of the embeddings produced by the image encoder or text encoder)\n",
        "        projection_dim=CFG.projection_dim, # dimensionality of the space to which the embeddings will be projected\n",
        "        dropout=CFG.dropout # prevent overfitting\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim) # Fully connected layer, projects from embedding_dim to projection_dim\n",
        "        self.gelu = nn.GELU() # nonlinear activation function. Allows network to model complex relationships between the input and output.\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim) # Fully connected layer, projects intermediate space back to the same dimensionality\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout randomly sets some elements of the tensor to zero during training, helping the network generalize better by preventing overfitting.\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim) # helps smooth out the values and avoid exploding/vanishing gradients during training.\n",
        "\n",
        "    def forward(self, x): #  defines how the input embeddings x flow through the layers of the projection head.\n",
        "        projected = self.projection(x) #  input x is projected to a lower-dimensional space (projection_dim) using the first fully connected layer.\n",
        "        x = self.gelu(projected) # projected output is passed through the GELU activation function to introduce non-linearity.\n",
        "        x = self.fc(x) #  fully connected layer is applied to the output of GELU, maintaining the same dimensionality (projection_dim).\n",
        "        x = self.dropout(x) # Dropout randomly sets some elements to zero. Helps with regularization and reduces overfitting.\n",
        "        x = x + projected # Residual connection is added: original projection (projected) is added to the output of the fully connected layer.\n",
        "                          # Stabilizes training and prevents gradient issues. Allows model to learn differences from the original input rather than completely transforming it.\n",
        "        x = self.layer_norm(x) #  output is normalized across the feature dimension using layer normalization to ensure smooth training\n",
        "        return x # returns the final transformed embedding, which is now in the projection_dim space and ready for downstream tasks (e.g., contrastive loss)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ef99ce-6446-4ec9-8113-a8786c4b2e02",
      "metadata": {
        "id": "b0ef99ce-6446-4ec9-8113-a8786c4b2e02"
      },
      "outputs": [],
      "source": [
        "### CLASS: Clip Model\"\"\"\n",
        "\n",
        "# Encode both images and text into a shared embedding space, then calculate the contrastive loss between the two.\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__( # Constructor\n",
        "        self,\n",
        "        temperature=CFG.temperature, # scalar value used to scale the logits (similarities) before applying softmax.\n",
        "                                     #It controls the sharpness of the output distribution.\n",
        "                                     #A lower temperature sharpens the distribution, making high similarities more dominant.\n",
        "        image_embedding=CFG.image_embedding, # Dimensionality of the image embeddings (i.e., the size of the vector produced by the ImageEncoder).\n",
        "        text_embedding=CFG.text_embedding, # Dimensionality of the text embeddings (i.e., the size of the vector produced by the TextEncoder).\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder() # generates feature embeddings for the input images.\n",
        "        self.text_encoder = TextEncoder() # generates feature embeddings for the input text.\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding) # takes output from img encoder and projects into low-dim space for contrastive learning\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding) # similarly for text\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Input to model: batch, a dictionary containint image, input ids and attention mask\n",
        "        image_features = self.image_encoder(batch[\"image\"]) # batch of input images.\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"] # tokenized text sequences and corresponding attention masks.\n",
        "        )\n",
        "\n",
        "        # Getting Image and Text Embeddings (with same dimension)\n",
        "        image_embeddings = self.image_projection(image_features) # project image features.\n",
        "        text_embeddings = self.text_projection(text_features) # project text features.\n",
        "\n",
        "        # Calculating Constrastive Loss:\n",
        "        # 1. Compute similarity scores between the projected text embeddings and the image embeddings using matrix multiplication (@ operator).\n",
        "        # The dot product between every text embedding and every image embedding is computed to create the logits matrix.\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature # similarity matrix: row = text, column = image.\n",
        "\n",
        "        # 2. Identity matrix as target for cross-entropy loss\n",
        "        batch_size = logits.shape[0]  # This should be the same for both text and image logits\n",
        "        targets = torch.eye(batch_size, device=logits.device)  # Ensure batch size consistency\n",
        "\n",
        "\n",
        "        # 3. Calculate loss using cross-entropy\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "\n",
        "        # 4. Final loss as the average of the image and text losses, ensuring symmetry between the two modalities.\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "\n",
        "        return loss.mean() # The mean loss is returned for backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee3a397-81ee-464e-a573-89ce7473c41e",
      "metadata": {
        "id": "aee3a397-81ee-464e-a573-89ce7473c41e"
      },
      "outputs": [],
      "source": [
        "### FUNCTION: Custom cross entropy loss\"\"\"\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    # preds: Predicted values (logits) from the model.\n",
        "    #       These are unnormalized scores (typically before applying softmax) that represent the model's confidence in each class.\n",
        "    # targets: Identity matrix representing the ground truth (matching pairs).\n",
        "    # reduction='none': Specifies how to reduce (aggregate) the loss across the batch.\n",
        "    #                   none: No reduction, the per-sample loss is returned.\n",
        "    #                   mean: The loss is averaged across all samples in the batch.\n",
        "\n",
        "    # Create a LogSoftmax layer that applies the log of the softmax function along the last dimension (dim=-1)\n",
        "    # Softmax converts logits (unnormalized model predictions) into probabilities.\n",
        "    # LogSoftmax gives the natural logarithm of these probabilities, which is useful for computing log-likelihood-based losses.\n",
        "    # It is numerically more stable than computing softmax followed by a logarithm\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    # Calculate the loss\n",
        "    # Since the targets will be an identity matrix, this computes the negative log likelihood\n",
        "    # for the matching pairs (diagonal entries) and considers non-matching pairs (off-diagonal\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "\n",
        "    # Handling Reduction\n",
        "    if reduction == \"none\": # function returns the individual loss values for each sample in the batch (i.e., no aggregation is applied).\n",
        "        return loss\n",
        "    elif reduction == \"mean\": # function returns the mean of the individual loss values across the batch.\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2e8fab-badf-44c4-8ed3-5e0cea6fe81c",
      "metadata": {
        "id": "ff2e8fab-badf-44c4-8ed3-5e0cea6fe81c"
      },
      "outputs": [],
      "source": [
        "### FUNCTION: Create data loaders \"\"\"\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    # dataframe: Pandas DataFrame containing image paths and classes\n",
        "    # tokenizer: tokenizer\n",
        "    # mode: whether the DataLoader is being created for training or validation. Determines the behavior for data augmentation and shuffling.\n",
        "\n",
        "    # Call function that returns different image transformations based on whether the model is in:\n",
        "    #     training mode\n",
        "    #     validation mode\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    # Print DataFrame contents to check image and class values\n",
        "    print(f\"Building loaders with {len(dataframe)} samples.\")\n",
        "    print(f\"Sample images: {dataframe['image'].head()}\")\n",
        "    print(f\"Sample classes: {dataframe['classes'].head()}\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values, # pass the array of image file paths from the DataFrame.\n",
        "        dataframe[\"classes\"].values, # pass the array of classes from the DataFrame.\n",
        "        tokenizer=tokenizer, # to tokenize classes\n",
        "        transforms=transforms, # image transformations, specific to the mode (train/validation), are applied to the images in the dataset.\n",
        "    )\n",
        "\n",
        "    # Debugging output after dataset creation\n",
        "    print(f\"Dataset created with {len(dataset)} samples.\")\n",
        "\n",
        "    # Create a PyTorch DataLoader\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, # dataset object created above\n",
        "        batch_size=CFG.batch_size, # batch size for loading the data\n",
        "        num_workers=CFG.num_workers, # number of worker processes used to load the data in parallel. More workers = faster data loading (depends on the system’s hardware).\n",
        "        shuffle=True if mode == \"train\" else False, # Shuffling is important during training to ensure that the model doesn't learn the order of the data\n",
        "    )\n",
        "\n",
        "    # # Log some information about the dataloader\n",
        "    # logging.info(f'{mode.capitalize()} DataLoader created with {len(dataset)} samples, batch size {CFG.batch_size}, and {CFG.num_workers} workers.')\n",
        "    print(\"DataLoader created.\")\n",
        "    return dataloader # returns the DataLoader, which can then be used in the training/validation loop to load batches of images and classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49583fa-868e-43d8-bb06-9041bb1a9105",
      "metadata": {
        "id": "b49583fa-868e-43d8-bb06-9041bb1a9105"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f969d3-72a1-4af5-8021-039c3ec09ab4",
      "metadata": {
        "id": "03f969d3-72a1-4af5-8021-039c3ec09ab4"
      },
      "outputs": [],
      "source": [
        "text_encoder = TextEncoder(pretrained=True).to(CFG.device)  # Ensure it's on the correct device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6817abdd-917e-4e0f-abea-0dbd6469824b",
      "metadata": {
        "id": "6817abdd-917e-4e0f-abea-0dbd6469824b"
      },
      "outputs": [],
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c472d730-bc88-4dbe-a654-0b6b54170c3b",
      "metadata": {
        "id": "c472d730-bc88-4dbe-a654-0b6b54170c3b"
      },
      "outputs": [],
      "source": [
        "model = CLIPModel().to(CFG.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39ae389-88d8-49d3-ba26-171e4c2f92b1",
      "metadata": {
        "id": "d39ae389-88d8-49d3-ba26-171e4c2f92b1"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11fdfd2-2461-4888-9ffe-7d7dcf511417",
      "metadata": {
        "id": "e11fdfd2-2461-4888-9ffe-7d7dcf511417"
      },
      "source": [
        "## From COCOStats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e9bcfe-9b30-4fe3-97c1-587b99f9db64",
      "metadata": {
        "id": "e5e9bcfe-9b30-4fe3-97c1-587b99f9db64"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af06e091-acff-41ab-823b-7e06f8aa47a2",
      "metadata": {
        "id": "af06e091-acff-41ab-823b-7e06f8aa47a2"
      },
      "outputs": [],
      "source": [
        "text_encoder = TextEncoder(pretrained=True).to(CFG.device)  # Ensure it's on the correct device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5a97c-2f66-4555-b3df-b4e061b33b92",
      "metadata": {
        "id": "b4c5a97c-2f66-4555-b3df-b4e061b33b92"
      },
      "source": [
        "## COCO specific preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d056a65-7088-4a0b-9361-060301550394",
      "metadata": {
        "id": "9d056a65-7088-4a0b-9361-060301550394"
      },
      "outputs": [],
      "source": [
        "# Load COCO instance annotations using the COCO class of pycocotools library\n",
        "coco = COCO('path to file instances_val2017.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ca672e-8e94-40dd-8e5b-e075378990d4",
      "metadata": {
        "id": "06ca672e-8e94-40dd-8e5b-e075378990d4"
      },
      "source": [
        "# DEBUGGING category_id_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12d67bc-7e67-435a-87ab-7cb0d473bed1",
      "metadata": {
        "id": "b12d67bc-7e67-435a-87ab-7cb0d473bed1"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of to map category IDs to names\n",
        "category_id_to_name = {category['id']: category['name'] for category in coco.loadCats(coco.getCatIds())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de184fe-3789-49fc-803a-401f46f12f41",
      "metadata": {
        "id": "1de184fe-3789-49fc-803a-401f46f12f41"
      },
      "outputs": [],
      "source": [
        "# # Print all category IDs and names\n",
        "# for category_id, category_name in category_id_to_name.items():\n",
        "#     print(f\"ID: {category_id}, Name: {category_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edae45da-4a8f-45ca-a740-77ddb8c42531",
      "metadata": {
        "id": "edae45da-4a8f-45ca-a740-77ddb8c42531"
      },
      "outputs": [],
      "source": [
        "# Count the number of unique categories\n",
        "num_categories = len(category_id_to_name)\n",
        "print(f\"Total number of categories: {num_categories}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cb9e65-6e40-4eaf-9966-ad7ec9e7f984",
      "metadata": {
        "id": "08cb9e65-6e40-4eaf-9966-ad7ec9e7f984"
      },
      "outputs": [],
      "source": [
        "static_text = \"a photo of a \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e59ac6-f798-49d9-8fc5-2357bbb4b6c5",
      "metadata": {
        "id": "53e59ac6-f798-49d9-8fc5-2357bbb4b6c5"
      },
      "outputs": [],
      "source": [
        "# Create a list of class queries based on the category names\n",
        "class_queries = [f\"{static_text}{name}\" for name in category_id_to_name.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748480d7-67f0-4d4e-baaa-6a5736b42b8a",
      "metadata": {
        "id": "748480d7-67f0-4d4e-baaa-6a5736b42b8a"
      },
      "outputs": [],
      "source": [
        "print(class_queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b4e7a1-094d-4b4d-af4f-0629483124aa",
      "metadata": {
        "id": "82b4e7a1-094d-4b4d-af4f-0629483124aa"
      },
      "outputs": [],
      "source": [
        "class_embeddings = []\n",
        "for query in class_queries:\n",
        "    encoded_query = tokenizer([query], padding=True, truncation=True, return_tensors='pt').to(CFG.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_embedding = text_encoder(encoded_query[\"input_ids\"], encoded_query[\"attention_mask\"])\n",
        "        projected_embedding = model.text_projection(text_embedding).squeeze(0)  # Squeeze here\n",
        "\n",
        "        class_embeddings.append(projected_embedding)\n",
        "\n",
        "# Stack the embeddings into a single tensor\n",
        "class_embeddings_tensor = torch.stack(class_embeddings)  # Should now be [80, 256]\n",
        "class_embeddings_n = F.normalize(class_embeddings_tensor, p=2, dim=-1)  # Normalize after stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6d4fdb-64d4-4dd8-aef7-5d4b27b6d8e8",
      "metadata": {
        "id": "0f6d4fdb-64d4-4dd8-aef7-5d4b27b6d8e8"
      },
      "outputs": [],
      "source": [
        "print(projected_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ebc8fd-ca39-4a9c-b31e-6f99c47c2786",
      "metadata": {
        "id": "f2ebc8fd-ca39-4a9c-b31e-6f99c47c2786"
      },
      "outputs": [],
      "source": [
        "print(class_embeddings_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9f6a94-d004-4e7e-ba4c-7a35ef7b0cff",
      "metadata": {
        "id": "2c9f6a94-d004-4e7e-ba4c-7a35ef7b0cff"
      },
      "outputs": [],
      "source": [
        "print(class_embeddings_n.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326d2633-bdc1-46ce-a376-f79142999456",
      "metadata": {
        "id": "326d2633-bdc1-46ce-a376-f79142999456"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678aaebf-ba31-4358-b294-b8a744c539ba",
      "metadata": {
        "id": "678aaebf-ba31-4358-b294-b8a744c539ba"
      },
      "outputs": [],
      "source": [
        "# Prepare a dictionary to collect image paths and their corresponding class labels\n",
        "image_data = {}\n",
        "\n",
        "# Collect image paths and their corresponding class labels\n",
        "for annotation in coco.loadAnns(coco.getAnnIds()): # This loop iterates through all annotations in the COCO dataset.\n",
        "    category_id = annotation['category_id']\n",
        "    label_name = category_id_to_name[category_id].strip().lower()  # Clean label name\n",
        "\n",
        "    image_id = annotation['image_id']\n",
        "    image_file_name = coco.loadImgs(image_id)[0]['file_name']\n",
        "\n",
        "    if image_file_name not in image_data:\n",
        "        image_data[image_file_name] = set()  # Initialize as set\n",
        "\n",
        "    image_data[image_file_name].add(label_name)  # Add cleaned class name to set\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "coco_df = pd.DataFrame({\n",
        "    'image': list(image_data.keys()),\n",
        "    'classes': [','.join(sorted(classes)) for classes in image_data.values()]  # Join cleaned classes into a single string\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf05a357-14de-40fa-9f29-a4370510e86e",
      "metadata": {
        "id": "bf05a357-14de-40fa-9f29-a4370510e86e"
      },
      "outputs": [],
      "source": [
        "# Check the number of unique COCO classes\n",
        "num_classes = len(category_id_to_name)\n",
        "print(f\"Number of COCO classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c6d39e-a3c2-467f-9f75-fecccc6de2df",
      "metadata": {
        "id": "f2c6d39e-a3c2-467f-9f75-fecccc6de2df"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of images in coco_df: {len(coco_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8fb6db-24f9-474d-9dac-cc6a73b2cb18",
      "metadata": {
        "id": "df8fb6db-24f9-474d-9dac-cc6a73b2cb18"
      },
      "outputs": [],
      "source": [
        "# Filter coco_df to only include images with classes\n",
        "coco_df = coco_df[coco_df['classes'].str.strip().astype(bool)]\n",
        "print(f\"Number of images with ground truth: {len(coco_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307f3c0b-49ad-4d1c-85a2-32fb1a19c9a7",
      "metadata": {
        "id": "307f3c0b-49ad-4d1c-85a2-32fb1a19c9a7"
      },
      "outputs": [],
      "source": [
        "image_id = coco.getImgIds(imgIds=[\"000000389933.jpg\"])[0]\n",
        "annotations = coco.loadAnns(coco.getAnnIds(imgIds=image_id))\n",
        "ground_truth_labels = {category_id_to_name[annotation['category_id']] for annotation in annotations}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2f6b8d-50d2-4a88-a0da-dae40e6ecebd",
      "metadata": {
        "id": "dc2f6b8d-50d2-4a88-a0da-dae40e6ecebd"
      },
      "outputs": [],
      "source": [
        "print(annotations)  # This should match your expected ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4291d993-0e71-4753-bb89-a28884abbe0e",
      "metadata": {
        "id": "4291d993-0e71-4753-bb89-a28884abbe0e"
      },
      "source": [
        "## DEBUG DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c67582-4fa2-41de-b841-4ef4d9f472b6",
      "metadata": {
        "id": "f4c67582-4fa2-41de-b841-4ef4d9f472b6"
      },
      "outputs": [],
      "source": [
        "# View first 5 rows\n",
        "print(coco_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16957f68-c6bd-4030-8368-6ac1c156e3a5",
      "metadata": {
        "id": "16957f68-c6bd-4030-8368-6ac1c156e3a5"
      },
      "outputs": [],
      "source": [
        "# Get column names\n",
        "print(coco_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e790f7d-f0e8-4671-8123-9749877a5008",
      "metadata": {
        "id": "2e790f7d-f0e8-4671-8123-9749877a5008"
      },
      "outputs": [],
      "source": [
        "# Access specific column\n",
        "images = coco_df['image']\n",
        "print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814ad9a1-50d1-4634-a385-3c509799e518",
      "metadata": {
        "id": "814ad9a1-50d1-4634-a385-3c509799e518"
      },
      "outputs": [],
      "source": [
        "# Filter rows\n",
        "dog_rows = coco_df[coco_df['classes'].str.contains('dog')]\n",
        "print(dog_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240efde9-3b57-4cf8-b62f-9c792ace9d81",
      "metadata": {
        "id": "240efde9-3b57-4cf8-b62f-9c792ace9d81"
      },
      "outputs": [],
      "source": [
        "# Access classes of a certain image by position\n",
        "some_image_classes = coco_df.iloc[1]['classes']\n",
        "print(some_image_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5c72dc-2425-4794-ae9d-46b3574bcac4",
      "metadata": {
        "id": "2c5c72dc-2425-4794-ae9d-46b3574bcac4"
      },
      "outputs": [],
      "source": [
        "# Same by name\n",
        "image_name = \"000000289343.jpg\"\n",
        "image_classes = coco_df[coco_df['image'] == image_name]['classes'].values[0]\n",
        "print(image_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa72d9a-0d10-4680-82f1-86cdcc1b3814",
      "metadata": {
        "id": "1fa72d9a-0d10-4680-82f1-86cdcc1b3814"
      },
      "outputs": [],
      "source": [
        "# Get info\n",
        "print(coco_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c898d3-80fc-4112-85fd-444b911452fd",
      "metadata": {
        "id": "03c898d3-80fc-4112-85fd-444b911452fd"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate image names in the DataFrame\n",
        "duplicates = coco_df[coco_df.duplicated(subset='image', keep=False)]\n",
        "\n",
        "if not duplicates.empty:\n",
        "    print(\"Duplicate images found:\")\n",
        "    print(duplicates)\n",
        "else:\n",
        "    print(\"No duplicate images found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf7e99a-cd63-4e70-afe0-891b1dbf2741",
      "metadata": {
        "id": "8cf7e99a-cd63-4e70-afe0-891b1dbf2741"
      },
      "outputs": [],
      "source": [
        "print(coco_df[coco_df['image'] == image_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b904357-1f33-4cf0-9195-8373e5e4a08e",
      "metadata": {
        "id": "4b904357-1f33-4cf0-9195-8373e5e4a08e"
      },
      "source": [
        "## Displaying an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fcda493-682d-438a-9f6e-d898e093a3ca",
      "metadata": {
        "id": "3fcda493-682d-438a-9f6e-d898e093a3ca"
      },
      "outputs": [],
      "source": [
        "# # Function to display an image with its classes\n",
        "# def display_image_with_classes(image_file_name, coco_df):\n",
        "#     # Load the image\n",
        "#     image_path = f\"path file {image_file_name}\"\n",
        "#     image = Image.open(image_path)\n",
        "\n",
        "#     # Get the classes for the image\n",
        "#     classes = coco_df[coco_df['image'] == image_file_name]['classes'].values[0]\n",
        "\n",
        "#     print(f\"Image name: {image_file_name}\")\n",
        "\n",
        "#     # Display the image and its classes along with the image name\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     plt.imshow(image)\n",
        "#     plt.title(f\"Image: {image_file_name}\\nClasses: {classes}\")  # Include image name in the title\n",
        "#     plt.axis('off')  # Hide axes\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5828dbd-e7d8-47fe-a40c-230f04bc95f3",
      "metadata": {
        "id": "a5828dbd-e7d8-47fe-a40c-230f04bc95f3"
      },
      "outputs": [],
      "source": [
        "# # Example usage for the 4952 images in the DataFrame (0 to 4951)\n",
        "# if not coco_df.empty:\n",
        "#     test_image = coco_df.iloc[3]['image']\n",
        "#     display_image_with_classes(test_image, coco_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463c3c67-7dd8-40f9-ba5e-9bd127ac7ffd",
      "metadata": {
        "id": "463c3c67-7dd8-40f9-ba5e-9bd127ac7ffd"
      },
      "source": [
        "## Get image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebbd511-c8b4-4970-b6f4-589fdc94d065",
      "metadata": {
        "id": "3ebbd511-c8b4-4970-b6f4-589fdc94d065"
      },
      "outputs": [],
      "source": [
        "def get_image_embeddings(test_df, model_path):\n",
        "    print(\"Starting get_image_embeddings...\")\n",
        "    print(f\"Input DataFrame shape: {test_df.shape}\")\n",
        "\n",
        "    test_loader = build_loaders(test_df, tokenizer, mode=\"valid\")\n",
        "    print(\"DataLoader created.\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    test_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "           # print(f\"Processing batch with size: {batch['image'].size()}\")\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            test_image_embeddings.append(image_embeddings)\n",
        "\n",
        "    print(\"Image embeddings calculated.\")\n",
        "    embeddings_tensor = torch.cat(test_image_embeddings)\n",
        "    print(f\"Final embeddings shape: {embeddings_tensor.shape}\")\n",
        "\n",
        "    # Return the model, embeddings, and DataLoader\n",
        "    return model, embeddings_tensor, test_loader  # Ensure test_loader is included here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41ce255-12ed-442e-bc7a-4ae9f3c75b10",
      "metadata": {
        "id": "e41ce255-12ed-442e-bc7a-4ae9f3c75b10"
      },
      "outputs": [],
      "source": [
        "# Coco_df is the DataFrame containing the COCO validation images and their paths\n",
        "model, image_embeddings, test_loader = get_image_embeddings(coco_df, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc0b80d-1c11-4a8d-bcf1-7fe7d1e726eb",
      "metadata": {
        "id": "1cc0b80d-1c11-4a8d-bcf1-7fe7d1e726eb"
      },
      "source": [
        "## START of top-1 accuracy test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3375ba4e-e4be-4359-b22e-0c49a8621e05",
      "metadata": {
        "id": "3375ba4e-e4be-4359-b22e-0c49a8621e05"
      },
      "outputs": [],
      "source": [
        "# Initialize counters\n",
        "matches_found = 0\n",
        "correct_predictions = []  # List to store correct predictions\n",
        "incorrect_predictions = []  # List to store incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6b1bda-ce1c-4c55-aaa2-557ca086520b",
      "metadata": {
        "id": "fa6b1bda-ce1c-4c55-aaa2-557ca086520b"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(len(coco_df)), desc=\"Processing Images with In-Memory Embeddings\"):\n",
        "    image_name = coco_df.iloc[i]['image']\n",
        "\n",
        "    # Access the precomputed embedding from image_embeddings directly\n",
        "    image_embedding = image_embeddings[i].to(CFG.device)\n",
        "    image_embedding_n = F.normalize(image_embedding, p=2, dim=-1)  # Normalize embedding\n",
        "\n",
        "    # Calculate similarity with class embeddings\n",
        "    similarities = (100.0 * image_embedding_n @ class_embeddings_n.T).softmax(dim=-1)\n",
        "    index_of_match = torch.argmax(similarities, dim=-1).item()\n",
        "    category_name_found = class_queries[index_of_match][len(static_text):]\n",
        "\n",
        "    if category_name_found in coco_df.iloc[i]['classes']:\n",
        "        matches_found += 1\n",
        "        correct_predictions.append({\n",
        "            \"image\": image_name,\n",
        "            \"prediction\": category_name_found\n",
        "        })\n",
        "    else:\n",
        "        # Log incorrect predictions\n",
        "        incorrect_predictions.append({\n",
        "            \"image\": image_name,\n",
        "            \"predicted\": category_name_found,\n",
        "            \"actual\": coco_df.iloc[i]['classes']  # Actual classes for reference\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abc40f3-ee6a-49ca-a39f-a08c1f50d51c",
      "metadata": {
        "id": "4abc40f3-ee6a-49ca-a39f-a08c1f50d51c"
      },
      "outputs": [],
      "source": [
        "print(f\"Matches found: {matches_found}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60a4aba-0671-43f5-8526-27baa3006015",
      "metadata": {
        "id": "a60a4aba-0671-43f5-8526-27baa3006015"
      },
      "source": [
        "## END of top-1 accuracy test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09fad91f-7e3e-4b9e-a4a2-443f300fb7bb",
      "metadata": {
        "id": "09fad91f-7e3e-4b9e-a4a2-443f300fb7bb"
      },
      "source": [
        "## Count correct predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a5a87d-a30b-493b-bbc3-14bb2a445acc",
      "metadata": {
        "id": "d1a5a87d-a30b-493b-bbc3-14bb2a445acc"
      },
      "outputs": [],
      "source": [
        "coco_classes = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fd2065-1ba3-4f54-a005-9c1d64f15766",
      "metadata": {
        "id": "b2fd2065-1ba3-4f54-a005-9c1d64f15766"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to hold counts for each class\n",
        "class_counts = {class_name: 0 for class_name in coco_classes}\n",
        "\n",
        "# Count occurrences of each prediction\n",
        "for entry in correct_predictions:\n",
        "    prediction = entry[\"prediction\"]\n",
        "    if prediction in class_counts:\n",
        "        class_counts[prediction] += 1\n",
        "\n",
        "# Print the counts for each class\n",
        "for class_name, count in sorted(class_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d8b437-bc90-4687-9501-5b7453d4fa7a",
      "metadata": {
        "id": "60d8b437-bc90-4687-9501-5b7453d4fa7a"
      },
      "source": [
        "## Count incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d60c5e2-e5a0-4f0b-bee0-30e84110d5ef",
      "metadata": {
        "id": "8d60c5e2-e5a0-4f0b-bee0-30e84110d5ef"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to hold counts for each incorrect prediction category\n",
        "incorrect_class_counts = {class_name: 0 for class_name in coco_classes}\n",
        "\n",
        "# Count occurrences of each incorrect prediction\n",
        "for entry in incorrect_predictions:\n",
        "    prediction = entry[\"predicted\"]\n",
        "    if prediction in incorrect_class_counts:\n",
        "        incorrect_class_counts[prediction] += 1\n",
        "\n",
        "for class_name, count in sorted(incorrect_class_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ad9bbb-837b-400f-af81-c4f359b8d07f",
      "metadata": {
        "id": "08ad9bbb-837b-400f-af81-c4f359b8d07f"
      },
      "source": [
        "## Save information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db07fb1-de75-4b7f-a78f-13354b67fb88",
      "metadata": {
        "id": "2db07fb1-de75-4b7f-a78f-13354b67fb88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae27fe78-04b0-4615-8b8a-0fcedc63588b",
      "metadata": {
        "id": "ae27fe78-04b0-4615-8b8a-0fcedc63588b"
      },
      "outputs": [],
      "source": [
        "# Define the folder and file path\n",
        "output_folder = \"path to output folder\"\n",
        "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n",
        "# Set the file name and path\n",
        "output_path = os.path.join(output_folder, f\"{model_name}_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5391750a-1be5-4948-9adf-6107bb0fd701",
      "metadata": {
        "id": "5391750a-1be5-4948-9adf-6107bb0fd701"
      },
      "outputs": [],
      "source": [
        "# Data structure to hold results\n",
        "results = {\n",
        "    \"model\": model_name,\n",
        "    \"total_correct\": len(correct_predictions),\n",
        "    \"total_incorrect\": len(incorrect_predictions),\n",
        "    \"correct_predictions\": correct_predictions,\n",
        "    \"incorrect_predictions\": incorrect_predictions\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f4b31b-3840-4463-a17b-87a57d66bacc",
      "metadata": {
        "id": "00f4b31b-3840-4463-a17b-87a57d66bacc"
      },
      "outputs": [],
      "source": [
        "# Save the results as a JSON file\n",
        "with open(output_path, \"w\") as outfile:\n",
        "    json.dump(results, outfile, indent=4)\n",
        "\n",
        "print(f\"Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1d630d4-62ce-4981-9f57-533637a0c176",
      "metadata": {
        "id": "b1d630d4-62ce-4981-9f57-533637a0c176"
      },
      "source": [
        "## Check information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3314811e-3eb4-4309-b407-b4feba8b3fe7",
      "metadata": {
        "id": "3314811e-3eb4-4309-b407-b4feba8b3fe7"
      },
      "outputs": [],
      "source": [
        "# Path to the JSON file\n",
        "file_path = f\" path to results.json\"\n",
        "\n",
        "# Load and verify the JSON content\n",
        "with open(file_path, \"r\") as infile:\n",
        "    loaded_results = json.load(infile)\n",
        "\n",
        "# Display the loaded data to verify correctness\n",
        "print(\"Model Name:\", loaded_results[\"model\"])\n",
        "print(\"Total Correct Predictions:\", loaded_results[\"total_correct\"])\n",
        "print(\"Total Incorrect Predictions:\", loaded_results[\"total_incorrect\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96b23c3-de76-4cc8-9e9c-3e40d1e649b0",
      "metadata": {
        "id": "e96b23c3-de76-4cc8-9e9c-3e40d1e649b0"
      },
      "outputs": [],
      "source": [
        "# # Count occurrences of a category in correct predictions\n",
        "# target_category = \"bird\"\n",
        "\n",
        "# category_correct_count = sum(1 for entry in loaded_results[\"correct_predictions\"] if entry[\"prediction\"] == target_category)\n",
        "# category_incorrect_count = sum(1 for entry in loaded_results[\"incorrect_predictions\"] if entry[\"predicted\"] == target_category)\n",
        "\n",
        "# print(f\"The category '{target_category}' was correctly predicted {category_correct_count} times.\")\n",
        "# print(f\"The category '{target_category}' was incorrectly predicted {category_incorrect_count} times.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485e09d2-fdb8-4453-9184-2e51aa600cd4",
      "metadata": {
        "id": "485e09d2-fdb8-4453-9184-2e51aa600cd4"
      },
      "source": [
        "## Double check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8b77ec-5d5e-4339-8e92-892fac9c5beb",
      "metadata": {
        "id": "3f8b77ec-5d5e-4339-8e92-892fac9c5beb"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to hold counts for each correct prediction category\n",
        "correct_class_counts = defaultdict(int)\n",
        "\n",
        "# Count occurrences of each correct prediction\n",
        "for entry in loaded_results[\"correct_predictions\"]:\n",
        "    predicted_class = entry[\"prediction\"]\n",
        "    correct_class_counts[predicted_class] += 1\n",
        "\n",
        "# Display the counts for each correctly predicted class\n",
        "for class_name, count in sorted(correct_class_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880eb326-d05a-4594-bf6d-9f0c62d4c02f",
      "metadata": {
        "id": "880eb326-d05a-4594-bf6d-9f0c62d4c02f"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to hold counts for each incorrect prediction category\n",
        "incorrect_class_counts = defaultdict(int)\n",
        "\n",
        "# Count occurrences of each incorrect prediction\n",
        "for entry in loaded_results[\"incorrect_predictions\"]:\n",
        "    predicted_class = entry[\"predicted\"]\n",
        "    incorrect_class_counts[predicted_class] += 1\n",
        "\n",
        "# Display the counts for each incorrectly predicted class\n",
        "for class_name, count in sorted(incorrect_class_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ccde03-ed91-4ed1-a00c-91f936dfbe86",
      "metadata": {
        "id": "f7ccde03-ed91-4ed1-a00c-91f936dfbe86"
      },
      "source": [
        "## Generate images and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cad53f0-4ba9-48cc-82ff-bb9308b6f47c",
      "metadata": {
        "id": "9cad53f0-4ba9-48cc-82ff-bb9308b6f47c"
      },
      "outputs": [],
      "source": [
        "# Convert dictionaries to DataFrames\n",
        "correct_df = pd.DataFrame(list(correct_class_counts.items()), columns=['Category', 'Correct'])\n",
        "incorrect_df = pd.DataFrame(list(incorrect_class_counts.items()), columns=['Category', 'Incorrect'])\n",
        "\n",
        "# Merge data for easier plotting\n",
        "category_counts_df = pd.merge(correct_df, incorrect_df, on='Category', how='outer').fillna(0)\n",
        "category_counts_df = category_counts_df.sort_values(by=['Correct', 'Incorrect'], ascending=False)\n",
        "\n",
        "# Set a wider figure size\n",
        "plt.figure(figsize=(20, 8))  # Adjust width and height as needed\n",
        "\n",
        "# Plot\n",
        "category_counts_df.plot(kind='bar', x='Category', stacked=True, ax=plt.gca())\n",
        "plt.title(\"Correct vs. Incorrect Predictions per Category\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Prediction Count\")\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate category names for better readability\n",
        "\n",
        "# Save the plot as a PNG file, including the model name\n",
        "output_path = f\"path to save {model_name}_correct_vs_incorrect_predictions.png\"\n",
        "plt.savefig(output_path, format='png', bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "print(f\"Plot saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f55431-2efa-4dc0-b354-b50eb47bfb93",
      "metadata": {
        "id": "a4f55431-2efa-4dc0-b354-b50eb47bfb93"
      },
      "source": [
        "## Top 10 plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3af1e70-77ab-4729-977b-2909403f8cda",
      "metadata": {
        "id": "e3af1e70-77ab-4729-977b-2909403f8cda"
      },
      "outputs": [],
      "source": [
        "# Sort categories by the number of correct predictions and select the top 10\n",
        "top_correct_df = category_counts_df.sort_values(by='Correct', ascending=False).head(10)\n",
        "\n",
        "# Plot the top 10 categories with the highest number of correct predictions\n",
        "top_correct_df.plot(kind='bar', x='Category', y='Correct', color='green')\n",
        "plt.title(\"Top 10 Categories with Highest Number of Correct Predictions\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Correct Predictions\")\n",
        "\n",
        "# Save the plot as a PNG file, including the model name\n",
        "output_path = f\"path to save {model_name}_top_10_correct.png\"\n",
        "plt.savefig(output_path, format='png', bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "print(f\"Plot saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b74a6ec-3e80-47f4-a92b-2ed6f70b2b53",
      "metadata": {
        "id": "7b74a6ec-3e80-47f4-a92b-2ed6f70b2b53"
      },
      "outputs": [],
      "source": [
        "top_misclassified_df = category_counts_df.sort_values(by='Incorrect', ascending=False).head(10)\n",
        "\n",
        "top_misclassified_df.plot(kind='bar', x='Category', y='Incorrect', color='red')\n",
        "plt.title(\"Top 10 Most Misclassified Categories\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Incorrect Predictions\")\n",
        "\n",
        "# Save the plot as a PNG file, including the model name\n",
        "output_path = f\" path to save {model_name}_top_10_incorrect.png\"\n",
        "plt.savefig(output_path, format='png', bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "print(f\"Plot saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f14d5ce-a40d-4fc7-84ca-5b5e396a7b0b",
      "metadata": {
        "id": "3f14d5ce-a40d-4fc7-84ca-5b5e396a7b0b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}